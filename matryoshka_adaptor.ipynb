{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdg4efAmPQ32"
      },
      "source": [
        "## Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT1EJEfjPQ34"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNO5WkvKPQ35",
        "outputId": "decd2625-7e24-422e-e478-3f20bada4688"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# !pip install sentence-transformers\n",
        "# !pip install mteb\n",
        "# !pip install beir\n",
        "# !pip install datasets\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwnKu7oPQ36"
      },
      "source": [
        "## Matryoshka-Adaptor Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuuAdgX7sD00"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpi5vvq4sH80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define MatryoshkaAdaptor module - a simple MLP with skip connection\n",
        "class MatryoshkaAdaptor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch neural network module that adapts the output of an embedding model\n",
        "    into a desired output dimension using two linear transformations with a ReLU activation in between.\n",
        "    Includes a skip connection from input to output.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_output_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initializes the MatryoshkaAdaptor module.\n",
        "        \n",
        "        Args:\n",
        "            input_output_dim: An integer representing the input and output dimension of the module which are equal.\n",
        "            hidden_dim: An integer representing the hidden dimension of the module.\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(MatryoshkaAdaptor, self).__init__()\n",
        "        # First linear layer to transform the input dimension to a hidden dimension\n",
        "        self.linear1 = torch.nn.Linear(input_output_dim, hidden_dim)\n",
        "        # Second linear layer to transform the hidden dimension to the output dimension which is same as input dimension\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, input_output_dim)\n",
        "        # Activation function to introduce non-linearity\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        \"\"\"\n",
        "        Forward pass of the MatryoshkaAdaptor module.\n",
        "\n",
        "        Args:\n",
        "            embedding: A torch.Tensor of shape (batch_size, input_output_dim) representing the input embeddings.\n",
        "\n",
        "        Returns:\n",
        "            output: A torch.Tensor of shape (batch_size, input_output_dim) representing the matryoshka embeddings.\n",
        "        \"\"\"\n",
        "        # Apply the first linear transformation followed by the activation function\n",
        "        hidden_embedding = self.activation(self.linear1(embedding))\n",
        "        \n",
        "        # Apply the second linear transformation to get the final adapted embedding\n",
        "        adapted_embedding = self.linear2(hidden_embedding)\n",
        "        \n",
        "        # Add the skip connection by adding the original embedding to the adapted embedding\n",
        "        mat_embedding = adapted_embedding + embedding\n",
        "\n",
        "        return mat_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i3hyw53tnoz"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_inssTutsBl"
      },
      "outputs": [],
      "source": [
        "# Equation 1 in paper\n",
        "def pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims):\n",
        "    \"\"\"\n",
        "    Computes the pairwise similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the mean pairwise similarity loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # Original embeddings only need to be normalized and computed once: \n",
        "\n",
        "    # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "    normalized_ori_corpus_embeddings = F.normalize(ori_corpus_embeddings, p=2, dim=1)\n",
        "    # Compute the cosine similarity matrices\n",
        "    original_similarity_matrix = torch.matmul(normalized_ori_corpus_embeddings, normalized_ori_corpus_embeddings.T)\n",
        "\n",
        "    # Get the indices of the upper triangle of the matrices, excluding the diagonal\n",
        "    batch_size = ori_corpus_embeddings.size(0)\n",
        "    i, j = torch.triu_indices(batch_size, batch_size, offset=1)\n",
        "\n",
        "    # Compute the pairwise cosine similarities\n",
        "    original_pairwise_similarities = original_similarity_matrix[i, j]\n",
        "\n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce the matryoshka embeddings to m dimensions\n",
        "        reduced_mat_corpus_embeddings = mat_corpus_embeddings[:, :m]\n",
        "\n",
        "        # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "        normalized_mat_corpus_embeddings = F.normalize(mat_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute the cosine similarity matrices\n",
        "        matryoshka_similarity_matrix = torch.matmul(normalized_mat_corpus_embeddings, normalized_mat_corpus_embeddings.T)\n",
        "        \n",
        "        # Compute the pairwise cosine similarities\n",
        "        matryoshka_pairwise_similarities = matryoshka_similarity_matrix[i, j]\n",
        "        \n",
        "        # Compute the absolute difference between corresponding pairwise similarities\n",
        "        similarity_differences = torch.abs(original_pairwise_similarities - matryoshka_pairwise_similarities)\n",
        "        \n",
        "        # Sum up all the absolute differences to produce the final loss\n",
        "        loss += torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Equation 2 in paper\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k=10):\n",
        "    \"\"\"\n",
        "    Computes the top-k similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similarities to consider (default is 10).\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the top-k similarity loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalize the original embeddings to get cosine similarity\n",
        "    normalized_ori_corpus_embeddings = F.normalize(ori_corpus_embeddings, p=2, dim=1)\n",
        "    \n",
        "    # Compute the original cosine similarity matrix\n",
        "    original_similarity_matrix = torch.matmul(normalized_ori_corpus_embeddings, normalized_ori_corpus_embeddings.T)\n",
        "    \n",
        "    # Exclude self-similarity by setting the diagonal to a very low value\n",
        "    original_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "    \n",
        "    # For each embedding, get the top-k similarities for the original embeddings\n",
        "    original_topk_values, _ = torch.topk(original_similarity_matrix, k, dim=1)\n",
        "    \n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce the matryoshka embeddings to m dimensions\n",
        "        reduced_mat_corpus_embeddings = mat_corpus_embeddings[:, :m]\n",
        "\n",
        "        # Normalize the reduced matryoshka embeddings to get cosine similarity\n",
        "        normalized_mat_corpus_embeddings = F.normalize(reduced_mat_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute the cosine similarity matrix for the reduced embeddings\n",
        "        matryoshka_similarity_matrix = torch.matmul(normalized_mat_corpus_embeddings, normalized_mat_corpus_embeddings.T)\n",
        "        \n",
        "        # Exclude self-similarity by setting the diagonal to a very low value\n",
        "        matryoshka_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "        \n",
        "        # For each embedding, get the top-k similarities for the matryoshka embeddings\n",
        "        matryoshka_topk_values, _ = torch.topk(matryoshka_similarity_matrix, k, dim=1)\n",
        "        \n",
        "        # Compute the absolute difference between the top-k similarities\n",
        "        similarity_differences = torch.abs(original_topk_values - matryoshka_topk_values)\n",
        "        \n",
        "        # Sum up all the absolute differences to accumulate the final loss\n",
        "        loss += torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Equation 3 in paper\n",
        "def reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the reconstruction loss to ensure the matryoshka embeddings do not deviate\n",
        "    significantly from the original embeddings, and thus act as a regularizer.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        alpha: A reconstruction coefficient that controls the weight of the reconstruction term.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the reconstruction loss.\n",
        "    \"\"\"\n",
        "    # Compute the difference between original and matryoshka embeddings\n",
        "    diff = ori_corpus_embeddings - mat_corpus_embeddings\n",
        "    \n",
        "    # Compute the L2 norm of the difference\n",
        "    loss = torch.norm(diff, p=2, dim=1)\n",
        "    \n",
        "    # Return the mean loss over the batch, scaled by alpha\n",
        "    return alpha * loss.mean()\n",
        "\n",
        "\n",
        "# Equation 4 in paper\n",
        "def unsupervised_objective_fn_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims,\n",
        "                                   k=10, alpha=1.0, beta=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall unsupervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, and beta-scaled reconstruction loss.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the reconstruction loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    topk_loss = topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k)\n",
        "    pairwise_loss = pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims)\n",
        "    reg_loss = reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, beta)\n",
        "    \n",
        "    # Combine the losses with the given scaling factors\n",
        "    total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# Equation 5 in paper\n",
        "def matryoshka_ranking_loss(query_embeddings, corpus_embeddings, relevance_scores, m_dims, k=10):\n",
        "    \"\"\"\n",
        "    Computes the Matryoshka Ranking Loss using optimized matrix operations and normalization.\n",
        "    \n",
        "    Args:\n",
        "        query_embeddings (torch.Tensor): Query embeddings of shape (num_queries, embedding_dim).\n",
        "        corpus_embeddings (torch.Tensor): Corpus embeddings of shape (num_docs, embedding_dim).\n",
        "        relevance_scores (torch.Tensor): Relevance scores of shape (num_queries, num_docs).\n",
        "        m_dims (List[int]): List of reduced dimensionality values.\n",
        "        k (int): Number of top similar documents to consider for the loss.\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: The computed Matryoshka Ranking Loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce embeddings to m dimensions\n",
        "        reduced_query_embeddings = query_embeddings[:, :m]\n",
        "        reduced_corpus_embeddings = corpus_embeddings[:, :m]\n",
        "        \n",
        "        # Normalize the embeddings to unit vectors\n",
        "        reduced_query_embeddings = F.normalize(reduced_query_embeddings, p=2, dim=1)\n",
        "        reduced_corpus_embeddings = F.normalize(reduced_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute cosine similarities\n",
        "        similarities = torch.matmul(reduced_query_embeddings, reduced_corpus_embeddings.T)\n",
        "        \n",
        "        # Get the top k most similar documents for each query\n",
        "        top_k_similarities, top_k_indices = torch.topk(similarities, k, dim=1, largest=True)\n",
        "        \n",
        "        # Gather the corresponding relevance scores\n",
        "        top_k_relevance = torch.gather(relevance_scores, 1, top_k_indices)\n",
        "        \n",
        "        # Calculate pairwise differences for the top-k relevance scores and similarities\n",
        "        relevance_diff = top_k_relevance.unsqueeze(2) - top_k_relevance.unsqueeze(1)\n",
        "        sim_diff = top_k_similarities.unsqueeze(2) - top_k_similarities.unsqueeze(1)\n",
        "        \n",
        "        # Only consider pairs where the relevance score difference is positive\n",
        "        positive_diff_mask = (relevance_diff > 0).float()\n",
        "        \n",
        "        # Compute the logistic loss\n",
        "        log_loss = torch.log(1 + torch.exp(sim_diff))\n",
        "        \n",
        "        # Weight the loss by the relevance difference and accumulate\n",
        "        weighted_loss = positive_diff_mask * relevance_diff * log_loss\n",
        "        loss += weighted_loss.sum()\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Equation 6 in paper\n",
        "def supervised_objective_fn_loss(ori_query_embeddings, ori_corpus_embeddings, mat_query_embeddings, mat_corpus_embeddings, relevance_scores, m_dims,\n",
        "                                   k=5, alpha=1.0, beta=1.0, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall supervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, beta-scaled reconstruction loss and gamma-scaled matryoshka ranking loss.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka embeddings.\n",
        "        relevance_scores: A tensor of shape (batch_size, num_docs) representing the relevance scores.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the reconstruction loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    topk_loss = topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k)\n",
        "    pairwise_loss = pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims)\n",
        "    reg_loss = reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, beta)\n",
        "    ranking_loss = matryoshka_ranking_loss(ori_corpus_embeddings, mat_corpus_embeddings, relevance_scores, m_dims)\n",
        "    ranking_loss = 0.0\n",
        "\n",
        "    # Combine the losses with the given scaling factors\n",
        "    total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss + gamma * ranking_loss\n",
        "    \n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Function\n",
        "\n",
        "This train function is designed for both unsupervised and supervised methods to train the Matryoshka Adaptor (not the embedding model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "\n",
        "def train(model, mat_adaptor, train_loader, loss_fn, config):\n",
        "    \"\"\"\n",
        "    Trains the MatryoshkaAdaptor module using the provided training data.\n",
        "\n",
        "    Args:\n",
        "        model: A SentenceTransformer model to generate embeddings.\n",
        "        mat_adaptor: A MatryoshkaAdaptor module to adapt the embeddings.\n",
        "        train_loader: A DataLoader object for the training dataset.\n",
        "        loss_fn: A loss function to compute the loss between original and matryoshka embeddings.\n",
        "        kwargs: A dictionary containing hyperparameters for training.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Detect if CUDA is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Move the model and mat_adaptor to the device\n",
        "    model.to(device)\n",
        "    mat_adaptor.to(device)\n",
        "\n",
        "    # Unpack the hyperparameters\n",
        "    epochs = config.get('epochs', 5)\n",
        "    lr = config.get('lr', 1e-3)\n",
        "    k = config.get('k', 10)  # Top-k similarity loss\n",
        "    m_dims = config.get('m_dims', [64, 128, 256])  # Matryoshka embedding dimensions\n",
        "    alpha = config.get('alpha', 1.0)  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    beta = config.get('beta', 1.0)  # Reconstruction loss scaling factor (beta in paper)\n",
        "    gamma = config.get('gamma', 1.0)  # Ranking loss scaling factor (gamma in paper)\n",
        "\n",
        "    # Initialize Weights & Biases\n",
        "    if config.get('wandb', False):\n",
        "        wandb.init(project=\"matryoshka-training\", config=config)\n",
        "        config = wandb.config\n",
        "\n",
        "    # Define an optimizer for the MatryoshkaAdaptor parameters\n",
        "    optimizer = Adam(mat_adaptor.parameters(), lr=lr)\n",
        "\n",
        "    # Set embedding model to eval mode (so that gradients only apply to the MatryoshkaAdaptor)\n",
        "    model.eval()\n",
        "    \n",
        "    # Set MatryoshkaAdaptor to training mode\n",
        "    mat_adaptor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            # Unsupervised training, so no labels\n",
        "            if isinstance(batch, list):\n",
        "                \n",
        "                # Generate embeddings for both texts\n",
        "                ori_embeddings = model.encode(batch, convert_to_tensor=True).to(device)  # model batched embeddings\n",
        "\n",
        "                # Forward pass embedding through the MatryoshkaAdaptor\n",
        "                mat_embeddings = mat_adaptor(ori_embeddings)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(ori_embeddings, mat_embeddings, m_dims, k=k, alpha=alpha, beta=beta)\n",
        "\n",
        "            # Supervised training expects dictionary with query, corpus and relevance scores\n",
        "            elif isinstance(batch, dict):\n",
        "                queries, corpus, relevance_scores = batch['query'], batch['corpus'], batch['relevance'].to(device)\n",
        "        \n",
        "                # Generate embeddings for both queries and corpus\n",
        "                ori_query_embeddings = model.encode(queries, convert_to_tensor=True).to(device)\n",
        "                ori_corpus_embeddings = model.encode(corpus, convert_to_tensor=True).to(device)\n",
        "\n",
        "                # Forward pass embeddings through the same MatryoshkaAdaptor\n",
        "                mat_query_embeddings = mat_adaptor(ori_query_embeddings).to(device)\n",
        "                mat_corpus_embeddings = mat_adaptor(ori_corpus_embeddings).to(device)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(ori_query_embeddings, ori_corpus_embeddings, mat_query_embeddings, mat_corpus_embeddings, relevance_scores, m_dims, k=k, alpha=alpha, beta=beta, gamma=gamma)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid batch format. Please provide a list or dictionary.\")\n",
        "            \n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            loss.backward()        # Compute gradients\n",
        "            optimizer.step()       # Update weights\n",
        "\n",
        "            print(F\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        \n",
        "        # Log the average loss to W&B\n",
        "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
        "        \n",
        "        # Print average loss for the epoch\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Finish the W&B run\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9GK1c80AGX"
      },
      "source": [
        "## Training of Adaptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsb8bFydrejN"
      },
      "source": [
        "### Unsupervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wfOS9tZEgZ0"
      },
      "source": [
        "#### Prepare Datasets \n",
        "\n",
        "We will use BEIR's NFCorpus, and train on the corpus only in an unsupervised manner, as detailed in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf7kTmMfsrUM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "corpus_ds = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
        "\n",
        "# Access the 'corpus' dataset\n",
        "dataset = corpus_ds['corpus']['text']\n",
        "\n",
        "# Define the split sizes\n",
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ccU_oZwyPQ37",
        "outputId": "8da10a58-7ccc-49bb-ef60-c1ff58a6a939"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'batch_size': 32,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm_dims': [64, 128, 256],  # Matryoshka embedding dimensions\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0,  # reconstruction loss scaling factor (beta in paper)\n",
        "    'wandb': False\n",
        "}\n",
        "\n",
        "# Create DataLoader for train and test datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=False)\n",
        "\n",
        "# train(model, mat_adaptor, train_dataloader, unsupervised_objective_fn_loss, hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Datasets\n",
        "\n",
        "As before, we will use BEIR's NFCorpus, and train on the corpus-query pairs in a supervised manner, as detailed in the paper.\n",
        "We need to manually download the BEIR dataset since the qrels cannot be accessed on the huggingface dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beir import util, LoggingHandler\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, handlers=[LoggingHandler()])\n",
        "\n",
        "# Define the dataset name and the path to store it\n",
        "dataset = \"nfcorpus\"\n",
        "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        "data_path = util.download_and_unzip(url, \"datasets\")\n",
        "\n",
        "# Load train and test data\n",
        "train_corpus, train_queries, train_qrels = GenericDataLoader(data_path).load(split=\"train\")\n",
        "dev_corpus, dev_queries, dev_qrels = GenericDataLoader(data_path).load(split=\"dev\")\n",
        "test_corpus, test_queries, test_qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
        "\n",
        "class BEIRDataset(Dataset):\n",
        "    def __init__(self, query, corpus, qrels):\n",
        "        self.query = query\n",
        "        self.corpus = corpus\n",
        "        self.qrels = qrels\n",
        "        self.query_ids = list(query.keys())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.query_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        query_id = self.query_ids[idx]\n",
        "        query = self.query[query_id]\n",
        "        \n",
        "        relevant_docs = self.qrels.get(query_id, {})\n",
        "        \n",
        "        # Get all document ids and relevance for this query\n",
        "        corpus_ids = list(relevant_docs.keys())\n",
        "        relevance = [relevant_docs[doc_id] for doc_id in corpus_ids]\n",
        "        \n",
        "        # Get document texts\n",
        "        corpus = [self.corpus[doc_id] for doc_id in corpus_ids]\n",
        "        \n",
        "        return {\n",
        "            'query_id': query_id,\n",
        "            'query': query,\n",
        "            'corpus_ids': corpus_ids,\n",
        "            'corpus': corpus,\n",
        "            'relevance': relevance\n",
        "        }\n",
        "    \n",
        "\n",
        "def collate_fn(batch):\n",
        "    query_ids = [item['query_id'] for item in batch]\n",
        "    query = [item['query'] for item in batch]\n",
        "    corpus_ids = [item['corpus_ids'] for item in batch]\n",
        "    corpus = [item['corpus'] for item in batch]\n",
        "    relevance = [item['relevance'] for item in batch]\n",
        "    \n",
        "    # Pad sequences if necessary\n",
        "    max_docs = max(len(docs) for docs in corpus_ids)\n",
        "    \n",
        "    padded_corpus_ids = [docs + [''] * (max_docs - len(docs)) for docs in corpus_ids]\n",
        "    padded_corpus = [texts + [''] * (max_docs - len(texts)) for texts in corpus]\n",
        "    padded_relevance = [s + [0] * (max_docs - len(s)) for s in relevance]\n",
        "    \n",
        "    return {\n",
        "        'query_ids': query_ids,\n",
        "        'query': query,\n",
        "        'corpus_ids': padded_corpus_ids,  # Keep as a list of lists\n",
        "        'corpus': padded_corpus,  # Keep as a list of lists\n",
        "        'relevance': torch.tensor(padded_relevance)  # Convert only relevance to tensor\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 5e-4,\n",
        "    'batch_size': 32,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm_dims': [64, 128, 256],  # Matryoshka embedding dimension\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0,  # reconstruction loss scaling factor (beta in paper)\n",
        "    'wandb': False,\n",
        "}\n",
        "\n",
        "train_dataset = BEIRDataset(train_queries, train_corpus, train_qrels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "# dev_dataset = BEIRDataset(dev_queries, dev_corpus, dev_qrels)\n",
        "# dev_dataloader = DataLoader(dev_dataset, batch_size=hyperparams['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# test_dataset = BEIRDataset(test_queries, test_corpus, test_qrels)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "train(model, mat_adaptor, train_dataloader, supervised_objective_fn_loss, hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhFgceqlt8xx"
      },
      "source": [
        "## BEIR Evaluation using NFCorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84bFC2eMSAQW"
      },
      "source": [
        "### Unmodified Model Performance for all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "LlrIjQBAs7b2",
        "outputId": "d7589094-fb96-42d5-b90a-59ca05231210"
      },
      "outputs": [],
      "source": [
        "import mteb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model_name = 'all-MiniLM-L6-v2_BASE'\n",
        "\n",
        "# Define the BEIR tasks you want to evaluate on\n",
        "tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "# Evaluate the model on the benchmark\n",
        "evaluation = mteb.MTEB(tasks=tasks)\n",
        "results = evaluation.run(model, output_folder=f\"results/{model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "class PCASentenceTransformer(SentenceTransformer):\n",
        "    \"\"\"\n",
        "    A SentenceTransformer model that applies PCA to reduce the dimensionality of the embeddings. \n",
        "    It serves as a wrapper to the inputted SentenceTransformer. \n",
        "    \"\"\"\n",
        "    def __init__(self, model_name_or_path, pca_components=128):\n",
        "        super().__init__(model_name_or_path)\n",
        "        self.pca_components = pca_components\n",
        "        self.pca = None\n",
        "\n",
        "    def fit_pca(self, embeddings):\n",
        "        \"\"\"Fits PCA on the provided embeddings.\"\"\"\n",
        "        self.pca = PCA(n_components=self.pca_components)\n",
        "        self.pca.fit(embeddings)\n",
        "\n",
        "    def encode(self, sentences, **kwargs):\n",
        "        \"\"\"Encodes the sentences and applies PCA to reduce dimensions.\"\"\"\n",
        "        # First, get the embeddings from the parent class method\n",
        "        embeddings = super().encode(sentences, **kwargs)\n",
        "\n",
        "        # If PCA is not fitted, fit it using the embeddings\n",
        "        if self.pca is None:\n",
        "            self.fit_pca(embeddings)\n",
        "\n",
        "        # Transform the embeddings using the fitted PCA model\n",
        "        reduced_embeddings = self.pca.transform(embeddings)\n",
        "        return reduced_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_model = PCASentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', pca_components=128)\n",
        "model_name = 'all-MiniLM-L6-v2_PCA_128'\n",
        "\n",
        "# Define the BEIR tasks you want to evaluate on\n",
        "tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "# Evaluate the model on the benchmark\n",
        "evaluation = mteb.MTEB(tasks=tasks)\n",
        "results = evaluation.run(pca_model, output_folder=f\"results/{model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + Unsupervised Matryoshka Adaptor"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
