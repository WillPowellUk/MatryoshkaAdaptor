{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdg4efAmPQ32"
      },
      "source": [
        "## Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT1EJEfjPQ34"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNO5WkvKPQ35",
        "outputId": "decd2625-7e24-422e-e478-3f20bada4688"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# !pip install sentence-transformers\n",
        "# !pip install mteb\n",
        "# !pip install beir\n",
        "# !pip install datasets\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwnKu7oPQ36"
      },
      "source": [
        "## Matryoshka-Adaptor Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuuAdgX7sD00"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpi5vvq4sH80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define MatryoshkaAdaptor module - a simple MLP with skip connection\n",
        "class MatryoshkaAdaptor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch neural network module that adapts the output of an embedding model\n",
        "    into a desired output dimension using two linear transformations with a ReLU activation in between.\n",
        "    Includes a skip connection from input to output.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_output_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initializes the MatryoshkaAdaptor module.\n",
        "        \n",
        "        Args:\n",
        "            input_output_dim: An integer representing the input and output dimension of the module which are equal.\n",
        "            hidden_dim: An integer representing the hidden dimension of the module.\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(MatryoshkaAdaptor, self).__init__()\n",
        "        # First linear layer to transform the input dimension to a hidden dimension\n",
        "        self.linear1 = torch.nn.Linear(input_output_dim, hidden_dim)\n",
        "        # Second linear layer to transform the hidden dimension to the output dimension which is same as input dimension\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, input_output_dim)\n",
        "        # Activation function to introduce non-linearity\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        \"\"\"\n",
        "        Forward pass of the MatryoshkaAdaptor module.\n",
        "\n",
        "        Args:\n",
        "            embedding: A torch.Tensor of shape (batch_size, input_output_dim) representing the input embeddings.\n",
        "\n",
        "        Returns:\n",
        "            output: A torch.Tensor of shape (batch_size, input_output_dim) representing the matryoshka embeddings.\n",
        "        \"\"\"\n",
        "        # Apply the first linear transformation followed by the activation function\n",
        "        hidden_embedding = self.activation(self.linear1(embedding))\n",
        "        \n",
        "        # Apply the second linear transformation to get the final adapted embedding\n",
        "        adapted_embedding = self.linear2(hidden_embedding)\n",
        "        \n",
        "        # Add the skip connection by adding the original embedding to the adapted embedding\n",
        "        mat_embedding = adapted_embedding + embedding\n",
        "\n",
        "        return mat_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i3hyw53tnoz"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_inssTutsBl"
      },
      "outputs": [],
      "source": [
        "# Equation 1 in paper\n",
        "def pairwise_similarity_loss(original_embeddings, matryoshka_embeddings):\n",
        "    \"\"\"\n",
        "    Computes the pairwise similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        original_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        matryoshka_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka embeddings.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the mean pairwise similarity loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "    normalized_original_embeddings = F.normalize(original_embeddings, p=2, dim=1)\n",
        "    normalized_matryoshka_embeddings = F.normalize(matryoshka_embeddings, p=2, dim=1)\n",
        "    \n",
        "    # Compute the cosine similarity matrices\n",
        "    original_similarity_matrix = torch.matmul(normalized_original_embeddings, normalized_original_embeddings.T)\n",
        "    matryoshka_similarity_matrix = torch.matmul(normalized_matryoshka_embeddings, normalized_matryoshka_embeddings.T)\n",
        "    \n",
        "    # Get the indices of the upper triangle of the matrices, excluding the diagonal\n",
        "    batch_size = original_embeddings.size(0)\n",
        "    i, j = torch.triu_indices(batch_size, batch_size, offset=1)\n",
        "    \n",
        "    # Compute the pairwise cosine similarities\n",
        "    original_pairwise_similarities = original_similarity_matrix[i, j]\n",
        "    matryoshka_pairwise_similarities = matryoshka_similarity_matrix[i, j]\n",
        "    \n",
        "    # Compute the absolute difference between corresponding pairwise similarities\n",
        "    similarity_differences = torch.abs(original_pairwise_similarities - matryoshka_pairwise_similarities)\n",
        "    \n",
        "    # Sum up all the absolute differences to produce the final loss\n",
        "    loss = torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Equation 2 in paper\n",
        "def topk_similarity_loss(original_embeddings, matryoshka_embeddings, k=5):\n",
        "    \"\"\"\n",
        "    Computes the top-k similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        original_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        matryoshka_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka embeddings.\n",
        "        k: The number of top similarities to consider (default is 5).\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the top-k similarity loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "    normalized_original_embeddings = F.normalize(original_embeddings, p=2, dim=1)\n",
        "    normalized_matryoshka_embeddings = F.normalize(matryoshka_embeddings, p=2, dim=1)\n",
        "    \n",
        "    # Compute the cosine similarity matrices\n",
        "    original_similarity_matrix = torch.matmul(normalized_original_embeddings, normalized_original_embeddings.T)\n",
        "    matryoshka_similarity_matrix = torch.matmul(normalized_matryoshka_embeddings, normalized_matryoshka_embeddings.T)\n",
        "    \n",
        "    # Exclude self-similarity by setting the diagonal to a very low value\n",
        "    batch_size = original_embeddings.size(0)\n",
        "    original_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "    matryoshka_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "    \n",
        "    # For each embedding, get the top-k similarities and their corresponding indices\n",
        "    original_topk_values, _ = torch.topk(original_similarity_matrix, k, dim=1)\n",
        "    matryoshka_topk_values, _ = torch.topk(matryoshka_similarity_matrix, k, dim=1)\n",
        "    \n",
        "    # Compute the absolute difference between the top-k similarities\n",
        "    similarity_differences = torch.abs(original_topk_values - matryoshka_topk_values)\n",
        "    \n",
        "    # Sum up all the absolute differences to produce the final loss\n",
        "    loss = torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Equation 3 in paper\n",
        "def regularization_loss(original_embeddings, matryoshka_embeddings, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the regularization loss to ensure the matryoshka embeddings do not deviate\n",
        "    significantly from the original embeddings.\n",
        "    \n",
        "    Args:\n",
        "        original_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        matryoshka_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka embeddings.\n",
        "        alpha: A regularization coefficient that controls the weight of the regularization term.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the regularization loss.\n",
        "    \"\"\"\n",
        "    # Compute the difference between original and matryoshka embeddings\n",
        "    diff = original_embeddings - matryoshka_embeddings\n",
        "    \n",
        "    # Compute the L2 norm of the difference\n",
        "    loss = torch.norm(diff, p=2, dim=1)\n",
        "    \n",
        "    # Return the mean loss over the batch, scaled by alpha\n",
        "    return alpha * loss.mean()\n",
        "\n",
        "\n",
        "# Equation 4 in paper\n",
        "def unsupervised_objective_fn_loss(original_embeddings, matryoshka_embeddings, \n",
        "                                   k=5, alpha=1.0, beta=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall unsupervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, and beta-scaled regularization loss.\n",
        "    \n",
        "    Args:\n",
        "        original_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        matryoshka_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka embeddings.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the regularization loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    topk_loss = topk_similarity_loss(original_embeddings, matryoshka_embeddings, k)\n",
        "    pairwise_loss = pairwise_similarity_loss(original_embeddings, matryoshka_embeddings)\n",
        "    reg_loss = regularization_loss(original_embeddings, matryoshka_embeddings, beta)\n",
        "    \n",
        "    # Combine the losses with the given scaling factors\n",
        "    total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# Equation 5 in paper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9GK1c80AGX"
      },
      "source": [
        "## Training of Adaptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsb8bFydrejN"
      },
      "source": [
        "### Unsupervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wfOS9tZEgZ0"
      },
      "source": [
        "#### Prepare Datasets + Dataloaders\n",
        "\n",
        "We will use BEIR's NFCorpus, and train on the corpus only in an unsupervised manner, as detailed in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf7kTmMfsrUM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "corpus_ds = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
        "\n",
        "# Access the 'corpus' dataset\n",
        "dataset = corpus_ds['corpus']['text']\n",
        "\n",
        "# Define the split sizes\n",
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32  # Adjust this as needed\n",
        "\n",
        "# Create DataLoader for train and test datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ccU_oZwyPQ37",
        "outputId": "8da10a58-7ccc-49bb-ef60-c1ff58a6a939"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "import wandb\n",
        "\n",
        "def train(model, mat_adaptor, train_loader, loss_fn, kwargs):\n",
        "    \"\"\"\n",
        "    Trains the MatryoshkaAdaptor module using the provided training data.\n",
        "\n",
        "    Args:\n",
        "        model: A SentenceTransformer model to generate embeddings.\n",
        "        mat_adaptor: A MatryoshkaAdaptor module to adapt the embeddings.\n",
        "        train_loader: A DataLoader object for the training dataset.\n",
        "        loss_fn: A loss function to compute the loss between original and matryoshka embeddings.\n",
        "        kwargs: A dictionary containing hyperparameters for training.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize Weights & Biases\n",
        "    wandb.init(project=\"matryoshka-training\", config=kwargs)\n",
        "    config = wandb.config\n",
        "\n",
        "    # Unpack the hyperparameters\n",
        "    epochs = config.get('epochs', 5)\n",
        "    lr = config.get('lr', 1e-3)\n",
        "    k = config.get('k', 5) # Top-k similarity loss\n",
        "    m = config.get('m', 128) # Matryoshka embedding dimension\n",
        "    alpha = config.get('alpha', 1.0) # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    beta = config.get('beta', 1.0)  # Regularization loss scaling factor (beta in paper)\n",
        "\n",
        "    # Define an optimizer for the MatryoshkaAdaptor parameters\n",
        "    optimizer = Adam(mat_adaptor.parameters(), lr=lr)\n",
        "    \n",
        "    # Set MatryoshkaAdaptor to training mode\n",
        "    mat_adaptor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            # Generate embeddings for both texts\n",
        "            ori_embeddings = model.encode(batch, convert_to_tensor=True)  # model batched embeddings\n",
        "\n",
        "            # Forward pass embedding through the MatryoshkaAdaptor\n",
        "            mat_embeddings = mat_adaptor(ori_embeddings)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(ori_embeddings, mat_embeddings, k=k, alpha=alpha, beta=beta)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            loss.backward()        # Compute gradients\n",
        "            optimizer.step()        # Update weights\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        \n",
        "        # Log the average loss to W&B\n",
        "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
        "        \n",
        "        # Print average loss for the epoch\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Finish the W&B run\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm': 128,  # Matryoshka embedding dimension\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0  # Regularization loss scaling factor (beta in paper)\n",
        "}\n",
        "\n",
        "train(model, mat_adaptor, train_dataloader, unsupervised_objective_fn_loss, hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Datasets + Dataloaders\n",
        "\n",
        "As before, we will use BEIR's NFCorpus, and train on the corpus-query pairs in a supervised manner, as detailed in the paper.\n",
        "We need to manually download the BEIR dataset since the qrels cannot be accessed on the huggingface dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-22 23:26:10,937 - Loading Corpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3633/3633 [00:00<00:00, 17025.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-22 23:26:11,197 - Loaded 3633 TRAIN Documents.\n",
            "2024-08-22 23:26:11,197 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
            "2024-08-22 23:26:11,198 - Loading Queries...\n",
            "2024-08-22 23:26:11,552 - Loaded 2590 TRAIN Queries.\n",
            "2024-08-22 23:26:11,552 - Query Example: Breast Cancer Cells Feed on Cholesterol\n",
            "2024-08-22 23:26:11,554 - Loading Corpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3633/3633 [00:00<00:00, 21010.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-22 23:26:11,752 - Loaded 3633 DEV Documents.\n",
            "2024-08-22 23:26:11,753 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
            "2024-08-22 23:26:11,754 - Loading Queries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-22 23:26:11,860 - Loaded 324 DEV Queries.\n",
            "2024-08-22 23:26:11,861 - Query Example: Why Deep Fried Foods May Cause Cancer\n",
            "2024-08-22 23:26:11,862 - Loading Corpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3633/3633 [00:00<00:00, 15800.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-22 23:26:12,121 - Loaded 3633 TEST Documents.\n",
            "2024-08-22 23:26:12,122 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
            "2024-08-22 23:26:12,122 - Loading Queries...\n",
            "2024-08-22 23:26:12,225 - Loaded 323 TEST Queries.\n",
            "2024-08-22 23:26:12,226 - Query Example: Do Cholesterol Statin Drugs Cause Breast Cancer?\n"
          ]
        }
      ],
      "source": [
        "from beir import util, LoggingHandler\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, handlers=[LoggingHandler()])\n",
        "\n",
        "# Define the dataset name and the path to store it\n",
        "dataset = \"nfcorpus\"\n",
        "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        "data_path = util.download_and_unzip(url, \"datasets\")\n",
        "\n",
        "# Load train and test data\n",
        "train_corpus, train_queries, train_qrels = GenericDataLoader(data_path).load(split=\"train\")\n",
        "dev_corpus, dev_queries, dev_qrels = GenericDataLoader(data_path).load(split=\"dev\")\n",
        "test_corpus, test_queries, test_qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
        "\n",
        "class BEIRDataset(Dataset):\n",
        "    def __init__(self, queries, corpus, qrels):\n",
        "        self.queries = queries\n",
        "        self.corpus = corpus\n",
        "        self.qrels = qrels\n",
        "        self.query_ids = list(queries.keys())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.query_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        query_id = self.query_ids[idx]\n",
        "        query_text = self.queries[query_id]\n",
        "        \n",
        "        relevant_docs = self.qrels.get(query_id, {})\n",
        "        \n",
        "        # Get all document ids and scores for this query\n",
        "        doc_ids = list(relevant_docs.keys())\n",
        "        scores = [relevant_docs[doc_id] for doc_id in doc_ids]\n",
        "        \n",
        "        # Get document texts\n",
        "        doc_texts = [self.corpus[doc_id] for doc_id in doc_ids]\n",
        "        \n",
        "        return {\n",
        "            'query_id': query_id,\n",
        "            'query_text': query_text,\n",
        "            'doc_ids': doc_ids,\n",
        "            'doc_texts': doc_texts,\n",
        "            'scores': scores\n",
        "        }\n",
        "    \n",
        "\n",
        "def collate_fn(batch):\n",
        "    query_ids = [item['query_id'] for item in batch]\n",
        "    query_texts = [item['query_text'] for item in batch]\n",
        "    doc_ids = [item['doc_ids'] for item in batch]\n",
        "    doc_texts = [item['doc_texts'] for item in batch]\n",
        "    scores = [item['scores'] for item in batch]\n",
        "    \n",
        "    # Pad sequences if necessary\n",
        "    max_docs = max(len(docs) for docs in doc_ids)\n",
        "    \n",
        "    padded_doc_ids = [docs + [''] * (max_docs - len(docs)) for docs in doc_ids]\n",
        "    padded_doc_texts = [texts + [''] * (max_docs - len(texts)) for texts in doc_texts]\n",
        "    padded_scores = [s + [0] * (max_docs - len(s)) for s in scores]\n",
        "    \n",
        "    return {\n",
        "        'query_ids': query_ids,\n",
        "        'query_texts': query_texts,\n",
        "        'doc_ids': padded_doc_ids,\n",
        "        'doc_texts': padded_doc_texts,\n",
        "        'scores': torch.tensor(padded_scores)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_dataset = BEIRDataset(train_queries, train_corpus, train_qrels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "dev_dataset = BEIRDataset(dev_queries, dev_corpus, dev_qrels)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = BEIRDataset(test_queries, test_corpus, test_qrels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-22 23:26:31,312 - NumExpr defaulting to 8 threads.\n",
            "2024-08-22 23:26:33,030 - PyTorch version 2.3.0 available.\n",
            "2024-08-22 23:26:33,039 - Polars version 1.5.0 available.\n",
            "2024-08-22 23:26:37,634 - Use pytorch device_name: cpu\n",
            "2024-08-22 23:26:37,634 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'MatryoshkaAdaptor' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_output_dim \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_sentence_embedding_dimension() \u001b[38;5;66;03m# Embedding dimension for model (d in paper)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m input_output_dim \u001b[38;5;66;03m# Let hidden layer dimension equal the embedding model dimension\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m mat_adaptor \u001b[38;5;241m=\u001b[39m \u001b[43mMatryoshkaAdaptor\u001b[49m(input_output_dim, hidden_dim)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MatryoshkaAdaptor' is not defined"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm': 128,  # Matryoshka embedding dimension\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0  # Regularization loss scaling factor (beta in paper)\n",
        "}\n",
        "\n",
        "train(model, mat_adaptor, train_dataloader, supervised_objective_fn_loss, hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhFgceqlt8xx"
      },
      "source": [
        "## BEIR Evaluation using NFCorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84bFC2eMSAQW"
      },
      "source": [
        "### Unmodified Model Performance for all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "LlrIjQBAs7b2",
        "outputId": "d7589094-fb96-42d5-b90a-59ca05231210"
      },
      "outputs": [],
      "source": [
        "import mteb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model_name = 'all-MiniLM-L6-v2_BASE'\n",
        "\n",
        "# Define the BEIR tasks you want to evaluate on\n",
        "tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "# Evaluate the model on the benchmark\n",
        "evaluation = mteb.MTEB(tasks=tasks)\n",
        "results = evaluation.run(model, output_folder=f\"results/{model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "class PCASentenceTransformer(SentenceTransformer):\n",
        "    \"\"\"\n",
        "    A SentenceTransformer model that applies PCA to reduce the dimensionality of the embeddings. \n",
        "    It serves as a wrapper to the inputted SentenceTransformer. \n",
        "    \"\"\"\n",
        "    def __init__(self, model_name_or_path, pca_components=128):\n",
        "        super().__init__(model_name_or_path)\n",
        "        self.pca_components = pca_components\n",
        "        self.pca = None\n",
        "\n",
        "    def fit_pca(self, embeddings):\n",
        "        \"\"\"Fits PCA on the provided embeddings.\"\"\"\n",
        "        self.pca = PCA(n_components=self.pca_components)\n",
        "        self.pca.fit(embeddings)\n",
        "\n",
        "    def encode(self, sentences, **kwargs):\n",
        "        \"\"\"Encodes the sentences and applies PCA to reduce dimensions.\"\"\"\n",
        "        # First, get the embeddings from the parent class method\n",
        "        embeddings = super().encode(sentences, **kwargs)\n",
        "\n",
        "        # If PCA is not fitted, fit it using the embeddings\n",
        "        if self.pca is None:\n",
        "            self.fit_pca(embeddings)\n",
        "\n",
        "        # Transform the embeddings using the fitted PCA model\n",
        "        reduced_embeddings = self.pca.transform(embeddings)\n",
        "        return reduced_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_model = PCASentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', pca_components=128)\n",
        "model_name = 'all-MiniLM-L6-v2_PCA_128'\n",
        "\n",
        "# Define the BEIR tasks you want to evaluate on\n",
        "tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "# Evaluate the model on the benchmark\n",
        "evaluation = mteb.MTEB(tasks=tasks)\n",
        "results = evaluation.run(pca_model, output_folder=f\"results/{model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + Unsupervised Matryoshka Adaptor"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
