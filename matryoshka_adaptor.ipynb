{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdg4efAmPQ32"
      },
      "source": [
        "## Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT1EJEfjPQ34"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNO5WkvKPQ35",
        "outputId": "decd2625-7e24-422e-e478-3f20bada4688"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# !pip install sentence-transformers\n",
        "# !pip install mteb\n",
        "# !pip install beir\n",
        "# !pip install datasets\n",
        "# !pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwnKu7oPQ36"
      },
      "source": [
        "## Matryoshka-Adaptor Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuuAdgX7sD00"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rpi5vvq4sH80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define MatryoshkaAdaptor module - a simple MLP with skip connection\n",
        "class MatryoshkaAdaptor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch neural network module that adapts the output of an embedding model\n",
        "    into a desired output dimension using two linear transformations with a ReLU activation in between.\n",
        "    Includes a skip connection from input to output.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_output_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initializes the MatryoshkaAdaptor module.\n",
        "        \n",
        "        Args:\n",
        "            input_output_dim: An integer representing the input and output dimension of the module which are equal.\n",
        "            hidden_dim: An integer representing the hidden dimension of the module.\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(MatryoshkaAdaptor, self).__init__()\n",
        "        # First linear layer to transform the input dimension to a hidden dimension\n",
        "        self.linear1 = torch.nn.Linear(input_output_dim, hidden_dim)\n",
        "        # Second linear layer to transform the hidden dimension to the output dimension which is same as input dimension\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, input_output_dim)\n",
        "        # Activation function to introduce non-linearity\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        \"\"\"\n",
        "        Forward pass of the MatryoshkaAdaptor module.\n",
        "\n",
        "        Args:\n",
        "            embedding: A torch.Tensor of shape (batch_size, input_output_dim) representing the input embeddings.\n",
        "\n",
        "        Returns:\n",
        "            output: A torch.Tensor of shape (batch_size, input_output_dim) representing the matryoshka embeddings.\n",
        "        \"\"\"\n",
        "        # Apply the first linear transformation followed by the activation function\n",
        "        hidden_embedding = self.activation(self.linear1(embedding))\n",
        "        \n",
        "        # Apply the second linear transformation to get the final adapted embedding\n",
        "        adapted_embedding = self.linear2(hidden_embedding)\n",
        "        \n",
        "        # Add the skip connection by adding the original embedding to the adapted embedding\n",
        "        mat_embedding = adapted_embedding + embedding\n",
        "\n",
        "        return mat_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i3hyw53tnoz"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X_inssTutsBl"
      },
      "outputs": [],
      "source": [
        "# Equation 1 in paper\n",
        "def pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims):\n",
        "    \"\"\"\n",
        "    Computes the pairwise similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the mean pairwise similarity loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # Original embeddings only need to be normalized and computed once: \n",
        "\n",
        "    # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "    normalized_ori_corpus_embeddings = F.normalize(ori_corpus_embeddings, p=2, dim=1)\n",
        "    # Compute the cosine similarity matrices\n",
        "    original_similarity_matrix = torch.matmul(normalized_ori_corpus_embeddings, normalized_ori_corpus_embeddings.T)\n",
        "\n",
        "    # Get the indices of the upper triangle of the matrices, excluding the diagonal\n",
        "    batch_size = ori_corpus_embeddings.size(0)\n",
        "    i, j = torch.triu_indices(batch_size, batch_size, offset=1)\n",
        "\n",
        "    # Compute the pairwise cosine similarities\n",
        "    original_pairwise_similarities = original_similarity_matrix[i, j]\n",
        "\n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce the matryoshka embeddings to m dimensions\n",
        "        reduced_mat_corpus_embeddings = mat_corpus_embeddings[:, :m]\n",
        "\n",
        "        # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "        normalized_mat_corpus_embeddings = F.normalize(mat_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute the cosine similarity matrices\n",
        "        matryoshka_similarity_matrix = torch.matmul(normalized_mat_corpus_embeddings, normalized_mat_corpus_embeddings.T)\n",
        "        \n",
        "        # Compute the pairwise cosine similarities\n",
        "        matryoshka_pairwise_similarities = matryoshka_similarity_matrix[i, j]\n",
        "        \n",
        "        # Compute the absolute difference between corresponding pairwise similarities\n",
        "        similarity_differences = torch.abs(original_pairwise_similarities - matryoshka_pairwise_similarities)\n",
        "        \n",
        "        # Sum up all the absolute differences to produce the final loss\n",
        "        loss += torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Equation 2 in paper\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k=10):\n",
        "    \"\"\"\n",
        "    Computes the top-k similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similarities to consider (default is 10).\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the top-k similarity loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalize the original embeddings to get cosine similarity\n",
        "    normalized_ori_corpus_embeddings = F.normalize(ori_corpus_embeddings, p=2, dim=1)\n",
        "    \n",
        "    # Compute the original cosine similarity matrix\n",
        "    original_similarity_matrix = torch.matmul(normalized_ori_corpus_embeddings, normalized_ori_corpus_embeddings.T)\n",
        "    \n",
        "    # Exclude self-similarity by setting the diagonal to a very low value\n",
        "    original_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "    \n",
        "    # For each embedding, get the top-k similarities for the original embeddings\n",
        "    original_topk_values, _ = torch.topk(original_similarity_matrix, k, dim=1)\n",
        "    \n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce the matryoshka embeddings to m dimensions\n",
        "        reduced_mat_corpus_embeddings = mat_corpus_embeddings[:, :m]\n",
        "\n",
        "        # Normalize the reduced matryoshka embeddings to get cosine similarity\n",
        "        normalized_mat_corpus_embeddings = F.normalize(reduced_mat_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute the cosine similarity matrix for the reduced embeddings\n",
        "        matryoshka_similarity_matrix = torch.matmul(normalized_mat_corpus_embeddings, normalized_mat_corpus_embeddings.T)\n",
        "        \n",
        "        # Exclude self-similarity by setting the diagonal to a very low value\n",
        "        matryoshka_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "        \n",
        "        # For each embedding, get the top-k similarities for the matryoshka embeddings\n",
        "        matryoshka_topk_values, _ = torch.topk(matryoshka_similarity_matrix, k, dim=1)\n",
        "        \n",
        "        # Compute the absolute difference between the top-k similarities\n",
        "        similarity_differences = torch.abs(original_topk_values - matryoshka_topk_values)\n",
        "        \n",
        "        # Sum up all the absolute differences to accumulate the final loss\n",
        "        loss += torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Equation 3 in paper\n",
        "def reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the reconstruction loss to ensure the matryoshka embeddings do not deviate\n",
        "    significantly from the original embeddings, and thus act as a regularizer.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        alpha: A reconstruction coefficient that controls the weight of the reconstruction term.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the reconstruction loss.\n",
        "    \"\"\"\n",
        "    # Compute the difference between original and matryoshka embeddings\n",
        "    diff = ori_corpus_embeddings - mat_corpus_embeddings\n",
        "    \n",
        "    # Compute the L2 norm of the difference\n",
        "    loss = torch.norm(diff, p=2, dim=1)\n",
        "    \n",
        "    # Return the mean loss over the batch, scaled by alpha\n",
        "    return alpha * loss.mean()\n",
        "\n",
        "\n",
        "# Equation 4 in paper\n",
        "def unsupervised_objective_fn_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims,\n",
        "                                   k=10, alpha=1.0, beta=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall unsupervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, and beta-scaled reconstruction loss.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the reconstruction loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    topk_loss = topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k)\n",
        "    pairwise_loss = pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims)\n",
        "    reg_loss = reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, beta)\n",
        "    \n",
        "    # Combine the losses with the given scaling factors\n",
        "    total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# Equation 5 in paper\n",
        "def matryoshka_ranking_loss(query_embeddings, corpus_embeddings, relevance_scores, m_dims, k=10):\n",
        "    \"\"\"\n",
        "    Computes the Matryoshka Ranking Loss using optimized matrix operations and normalization.\n",
        "    \n",
        "    Args:\n",
        "        query_embeddings (torch.Tensor): Query embeddings of shape (num_queries, embedding_dim).\n",
        "        corpus_embeddings (torch.Tensor): Corpus embeddings of shape (num_docs, embedding_dim).\n",
        "        relevance_scores (torch.Tensor): Relevance scores of shape (num_queries, num_docs).\n",
        "        m_dims (List[int]): List of reduced dimensionality values.\n",
        "        k (int): Number of top similar documents to consider for the loss.\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: The computed Matryoshka Ranking Loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce embeddings to m dimensions\n",
        "        reduced_query_embeddings = query_embeddings[:, :m]\n",
        "        reduced_corpus_embeddings = corpus_embeddings[:, :m]\n",
        "        \n",
        "        # Normalize the embeddings to unit vectors\n",
        "        reduced_query_embeddings = F.normalize(reduced_query_embeddings, p=2, dim=1)\n",
        "        reduced_corpus_embeddings = F.normalize(reduced_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute cosine similarities\n",
        "        similarities = torch.matmul(reduced_query_embeddings, reduced_corpus_embeddings.T)\n",
        "        \n",
        "        # Get the top k most similar documents for each query\n",
        "        top_k_similarities, top_k_indices = torch.topk(similarities, k, dim=1, largest=True)\n",
        "        \n",
        "        # Gather the corresponding relevance scores\n",
        "        top_k_relevance = torch.gather(relevance_scores, 1, top_k_indices)\n",
        "        \n",
        "        # Calculate pairwise differences for the top-k relevance scores and similarities\n",
        "        relevance_diff = top_k_relevance.unsqueeze(2) - top_k_relevance.unsqueeze(1)\n",
        "        sim_diff = top_k_similarities.unsqueeze(2) - top_k_similarities.unsqueeze(1)\n",
        "        \n",
        "        # Only consider pairs where the relevance score difference is positive\n",
        "        positive_diff_mask = (relevance_diff > 0).float()\n",
        "        \n",
        "        # Compute the logistic loss\n",
        "        log_loss = torch.log(1 + torch.exp(sim_diff))\n",
        "        \n",
        "        # Weight the loss by the relevance difference and accumulate\n",
        "        weighted_loss = positive_diff_mask * relevance_diff * log_loss\n",
        "        loss += weighted_loss.sum()\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Equation 6 in paper\n",
        "def supervised_objective_fn_loss(ori_query_embeddings, ori_corpus_embeddings, mat_query_embeddings, mat_corpus_embeddings, relevance_scores, m_dims,\n",
        "                                   k=5, alpha=1.0, beta=1.0, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall supervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, beta-scaled reconstruction loss and gamma-scaled matryoshka ranking loss.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka embeddings.\n",
        "        relevance_scores: A tensor of shape (batch_size, num_docs) representing the relevance scores.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the reconstruction loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    topk_loss = topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k)\n",
        "    pairwise_loss = pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims)\n",
        "    reg_loss = reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, beta)\n",
        "    # ranking_loss = matryoshka_ranking_loss(ori_corpus_embeddings, mat_corpus_embeddings, relevance_scores, m_dims)\n",
        "    ranking_loss = 0.0\n",
        "\n",
        "    # Combine the losses with the given scaling factors\n",
        "    total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss + gamma * ranking_loss\n",
        "    \n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Function\n",
        "\n",
        "This train function is designed for both unsupervised and supervised methods to train the Matryoshka Adaptor (not the embedding model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "\n",
        "def train(model, mat_adaptor, train_loader, loss_fn, config):\n",
        "    \"\"\"\n",
        "    Trains the MatryoshkaAdaptor module using the provided training data.\n",
        "\n",
        "    Args:\n",
        "        model: A SentenceTransformer model to generate embeddings.\n",
        "        mat_adaptor: A MatryoshkaAdaptor module to adapt the embeddings.\n",
        "        train_loader: A DataLoader object for the training dataset.\n",
        "        loss_fn: A loss function to compute the loss between original and matryoshka embeddings.\n",
        "        kwargs: A dictionary containing hyperparameters for training.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Detect if CUDA is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Move the model and mat_adaptor to the device\n",
        "    model.to(device)\n",
        "    mat_adaptor.to(device)\n",
        "\n",
        "    # Unpack the hyperparameters\n",
        "    epochs = config.get('epochs', 5)\n",
        "    lr = config.get('lr', 1e-3)\n",
        "    k = config.get('k', 10)  # Top-k similarity loss\n",
        "    m_dims = config.get('m_dims', [64, 128, 256])  # Matryoshka embedding dimensions\n",
        "    alpha = config.get('alpha', 1.0)  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    beta = config.get('beta', 1.0)  # Reconstruction loss scaling factor (beta in paper)\n",
        "    gamma = config.get('gamma', 1.0)  # Ranking loss scaling factor (gamma in paper)\n",
        "\n",
        "    # Initialize Weights & Biases\n",
        "    if config.get('wandb', False):\n",
        "        wandb.init(project=\"matryoshka-training\", config=config)\n",
        "        config = wandb.config\n",
        "\n",
        "    # Define an optimizer for the MatryoshkaAdaptor parameters\n",
        "    optimizer = Adam(mat_adaptor.parameters(), lr=lr)\n",
        "    \n",
        "    # Set MatryoshkaAdaptor to training mode\n",
        "    mat_adaptor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            # Unsupervised training, so no labels\n",
        "            if isinstance(batch, list):\n",
        "                \n",
        "                # Generate embeddings for both texts\n",
        "                ori_embeddings = model.encode(batch, convert_to_tensor=True).to(device)  # model batched embeddings\n",
        "\n",
        "                # Forward pass embedding through the MatryoshkaAdaptor\n",
        "                mat_embeddings = mat_adaptor(ori_embeddings)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(ori_embeddings, mat_embeddings, m_dims, k=k, alpha=alpha, beta=beta)\n",
        "\n",
        "            # Supervised training expects dictionary with query, corpus and relevance scores\n",
        "            elif isinstance(batch, dict):\n",
        "                queries, corpus, relevance_scores = batch['query'], batch['corpus'], batch['relevance'].to(device)\n",
        "        \n",
        "                # Generate embeddings for both queries and corpus\n",
        "                ori_query_embeddings = model.encode(queries, convert_to_tensor=True).to(device)\n",
        "                ori_corpus_embeddings = model.encode(corpus, convert_to_tensor=True).to(device)\n",
        "\n",
        "                # Forward pass embeddings through the same MatryoshkaAdaptor\n",
        "                mat_query_embeddings = mat_adaptor(ori_query_embeddings).to(device)\n",
        "                mat_corpus_embeddings = mat_adaptor(ori_corpus_embeddings).to(device)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(ori_query_embeddings, ori_corpus_embeddings, mat_query_embeddings, mat_corpus_embeddings, relevance_scores, m_dims, k=k, alpha=alpha, beta=beta, gamma=gamma)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid batch format. Please provide a list or dictionary.\")\n",
        "            \n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            loss.backward()        # Compute gradients\n",
        "            optimizer.step()       # Update weights\n",
        "\n",
        "            total_loss += loss.item()\n",
        "    \n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        \n",
        "        # Log the average loss to W&B\n",
        "        wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
        "        \n",
        "        # Print average loss for the epoch\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Finish the W&B run\n",
        "    wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9GK1c80AGX"
      },
      "source": [
        "## Training of Adaptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsb8bFydrejN"
      },
      "source": [
        "### Unsupervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wfOS9tZEgZ0"
      },
      "source": [
        "#### Prepare Datasets + Dataloaders\n",
        "\n",
        "We will use BEIR's NFCorpus, and train on the corpus only in an unsupervised manner, as detailed in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tf7kTmMfsrUM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "corpus_ds = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
        "\n",
        "# Access the 'corpus' dataset\n",
        "dataset = corpus_ds['corpus']['text']\n",
        "\n",
        "# Define the split sizes\n",
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32  # Adjust this as needed\n",
        "\n",
        "# Create DataLoader for train and test datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ccU_oZwyPQ37",
        "outputId": "8da10a58-7ccc-49bb-ef60-c1ff58a6a939"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm_dims': [64, 128, 256],  # Matryoshka embedding dimensions\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0,  # reconstruction loss scaling factor (beta in paper)\n",
        "    'wandb': False\n",
        "}\n",
        "\n",
        "train(model, mat_adaptor, train_dataloader, unsupervised_objective_fn_loss, hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Datasets + Dataloaders\n",
        "\n",
        "As before, we will use BEIR's NFCorpus, and train on the corpus-query pairs in a supervised manner, as detailed in the paper.\n",
        "We need to manually download the BEIR dataset since the qrels cannot be accessed on the huggingface dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:39,528 - Loading Corpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3633/3633 [00:00<00:00, 52323.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:39,614 - Loaded 3633 TRAIN Documents.\n",
            "2024-08-23 11:10:39,614 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
            "2024-08-23 11:10:39,614 - Loading Queries...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:39,814 - Loaded 2590 TRAIN Queries.\n",
            "2024-08-23 11:10:39,814 - Query Example: Breast Cancer Cells Feed on Cholesterol\n",
            "2024-08-23 11:10:39,814 - Loading Corpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3633/3633 [00:00<00:00, 51244.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:39,901 - Loaded 3633 DEV Documents.\n",
            "2024-08-23 11:10:39,901 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
            "2024-08-23 11:10:39,901 - Loading Queries...\n",
            "2024-08-23 11:10:39,943 - Loaded 324 DEV Queries.\n",
            "2024-08-23 11:10:39,943 - Query Example: Why Deep Fried Foods May Cause Cancer\n",
            "2024-08-23 11:10:39,944 - Loading Corpus...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3633/3633 [00:00<00:00, 54655.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:40,024 - Loaded 3633 TEST Documents.\n",
            "2024-08-23 11:10:40,024 - Doc Example: {'text': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995–2003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08–9.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38–0.55 and HR 0.54, 95% CI 0.44–0.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins’ effect on survival in breast cancer patients.', 'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}\n",
            "2024-08-23 11:10:40,024 - Loading Queries...\n",
            "2024-08-23 11:10:40,067 - Loaded 323 TEST Queries.\n",
            "2024-08-23 11:10:40,067 - Query Example: Do Cholesterol Statin Drugs Cause Breast Cancer?\n"
          ]
        }
      ],
      "source": [
        "from beir import util, LoggingHandler\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, handlers=[LoggingHandler()])\n",
        "\n",
        "# Define the dataset name and the path to store it\n",
        "dataset = \"nfcorpus\"\n",
        "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        "data_path = util.download_and_unzip(url, \"datasets\")\n",
        "\n",
        "# Load train and test data\n",
        "train_corpus, train_queries, train_qrels = GenericDataLoader(data_path).load(split=\"train\")\n",
        "dev_corpus, dev_queries, dev_qrels = GenericDataLoader(data_path).load(split=\"dev\")\n",
        "test_corpus, test_queries, test_qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
        "\n",
        "class BEIRDataset(Dataset):\n",
        "    def __init__(self, query, corpus, qrels):\n",
        "        self.query = query\n",
        "        self.corpus = corpus\n",
        "        self.qrels = qrels\n",
        "        self.query_ids = list(query.keys())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.query_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        query_id = self.query_ids[idx]\n",
        "        query = self.query[query_id]\n",
        "        \n",
        "        relevant_docs = self.qrels.get(query_id, {})\n",
        "        \n",
        "        # Get all document ids and relevance for this query\n",
        "        corpus_ids = list(relevant_docs.keys())\n",
        "        relevance = [relevant_docs[doc_id] for doc_id in corpus_ids]\n",
        "        \n",
        "        # Get document texts\n",
        "        corpus = [self.corpus[doc_id] for doc_id in corpus_ids]\n",
        "        \n",
        "        return {\n",
        "            'query_id': query_id,\n",
        "            'query': query,\n",
        "            'corpus_ids': corpus_ids,\n",
        "            'corpus': corpus,\n",
        "            'relevance': relevance\n",
        "        }\n",
        "    \n",
        "\n",
        "def collate_fn(batch):\n",
        "    query_ids = [item['query_id'] for item in batch]\n",
        "    query = [item['query'] for item in batch]\n",
        "    corpus_ids = [item['corpus_ids'] for item in batch]\n",
        "    corpus = [item['corpus'] for item in batch]\n",
        "    relevance = [item['relevance'] for item in batch]\n",
        "    \n",
        "    # Pad sequences if necessary\n",
        "    max_docs = max(len(docs) for docs in corpus_ids)\n",
        "    \n",
        "    padded_corpus_ids = [docs + [''] * (max_docs - len(docs)) for docs in corpus_ids]\n",
        "    padded_corpus = [texts + [''] * (max_docs - len(texts)) for texts in corpus]\n",
        "    padded_relevance = [s + [0] * (max_docs - len(s)) for s in relevance]\n",
        "    \n",
        "    return {\n",
        "        'query_ids': query_ids,\n",
        "        'query': query,\n",
        "        'corpus_ids': padded_corpus_ids,  # Keep as a list of lists\n",
        "        'corpus': padded_corpus,  # Keep as a list of lists\n",
        "        'relevance': torch.tensor(padded_relevance)  # Convert only relevance to tensor\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = BEIRDataset(train_queries, train_corpus, train_qrels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "dev_dataset = BEIRDataset(dev_queries, dev_corpus, dev_qrels)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = BEIRDataset(test_queries, test_corpus, test_qrels)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:40,101 - Use pytorch device_name: cuda\n",
            "2024-08-23 11:10:40,101 - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-23 11:10:41,394 - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwill63powell\u001b[0m (\u001b[33mwill63powell-Imperial College London\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.17.7 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/fsociety/Dev/MatryoshkaAdaptor/wandb/run-20240823_111042-y0pbz1qn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/will63powell-Imperial%20College%20London/matryoshka-training/runs/y0pbz1qn' target=\"_blank\">misunderstood-snow-9</a></strong> to <a href='https://wandb.ai/will63powell-Imperial%20College%20London/matryoshka-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/will63powell-Imperial%20College%20London/matryoshka-training' target=\"_blank\">https://wandb.ai/will63powell-Imperial%20College%20London/matryoshka-training</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/will63powell-Imperial%20College%20London/matryoshka-training/runs/y0pbz1qn' target=\"_blank\">https://wandb.ai/will63powell-Imperial%20College%20London/matryoshka-training/runs/y0pbz1qn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.21it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.80it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.60it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.61it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.88it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.84it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.22it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.36it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.34it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.16it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.14it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.80it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.84it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.71it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.47it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.08it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.60it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.31it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.71it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.09it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.34it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.12it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.43it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.38it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.62it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.82it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.59it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.31it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.59it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.05it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.89it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.69it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.85it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.55it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.69it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.31it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.82it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.73it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.53it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.46it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.89it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.62it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.67it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.00it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.09it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.69it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.64it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.48it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.96it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.21it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.24it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.91it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.40it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.44it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.41it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.98it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.99it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.65it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.32it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss: 681.7551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.16it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.25it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.14it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.35it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.27it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.03it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.85it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.23it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.41it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.75it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.38it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.87it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.81it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.33it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.49it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.44it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.07it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.23it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.19it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.82it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.17it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.44it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.15it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.54it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.73it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.12it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.13it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.45it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.96it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.85it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.60it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.72it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.05it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.12it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.91it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.75it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.67it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.32it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.38it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.54it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.69it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.48it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.84it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.90it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.77it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.05it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5, Loss: 645.9292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.86it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.93it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.42it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.80it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.48it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.23it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.55it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.69it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.96it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.06it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.33it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.91it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.00it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.91it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.87it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.03it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.58it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.03it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.98it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.72it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.59it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.74it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.99it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.10it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.96it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.94it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.20it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.88it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.79it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.83it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.43it/s]\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_adaptor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msupervised_objective_fn_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, mat_adaptor, train_loader, loss_fn, config)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Clear previous gradients\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[1;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()       \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     89\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm_dims': [64, 128, 256],  # Matryoshka embedding dimension\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0,  # reconstruction loss scaling factor (beta in paper)\n",
        "    'wandb': True,\n",
        "}\n",
        "\n",
        "train(model, mat_adaptor, train_dataloader, supervised_objective_fn_loss, hyperparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhFgceqlt8xx"
      },
      "source": [
        "## BEIR Evaluation using NFCorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84bFC2eMSAQW"
      },
      "source": [
        "### Unmodified Model Performance for all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "LlrIjQBAs7b2",
        "outputId": "d7589094-fb96-42d5-b90a-59ca05231210"
      },
      "outputs": [],
      "source": [
        "import mteb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model_name = 'all-MiniLM-L6-v2_BASE'\n",
        "\n",
        "# Define the BEIR tasks you want to evaluate on\n",
        "tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "# Evaluate the model on the benchmark\n",
        "evaluation = mteb.MTEB(tasks=tasks)\n",
        "results = evaluation.run(model, output_folder=f\"results/{model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "class PCASentenceTransformer(SentenceTransformer):\n",
        "    \"\"\"\n",
        "    A SentenceTransformer model that applies PCA to reduce the dimensionality of the embeddings. \n",
        "    It serves as a wrapper to the inputted SentenceTransformer. \n",
        "    \"\"\"\n",
        "    def __init__(self, model_name_or_path, pca_components=128):\n",
        "        super().__init__(model_name_or_path)\n",
        "        self.pca_components = pca_components\n",
        "        self.pca = None\n",
        "\n",
        "    def fit_pca(self, embeddings):\n",
        "        \"\"\"Fits PCA on the provided embeddings.\"\"\"\n",
        "        self.pca = PCA(n_components=self.pca_components)\n",
        "        self.pca.fit(embeddings)\n",
        "\n",
        "    def encode(self, sentences, **kwargs):\n",
        "        \"\"\"Encodes the sentences and applies PCA to reduce dimensions.\"\"\"\n",
        "        # First, get the embeddings from the parent class method\n",
        "        embeddings = super().encode(sentences, **kwargs)\n",
        "\n",
        "        # If PCA is not fitted, fit it using the embeddings\n",
        "        if self.pca is None:\n",
        "            self.fit_pca(embeddings)\n",
        "\n",
        "        # Transform the embeddings using the fitted PCA model\n",
        "        reduced_embeddings = self.pca.transform(embeddings)\n",
        "        return reduced_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_model = PCASentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', pca_components=128)\n",
        "model_name = 'all-MiniLM-L6-v2_PCA_128'\n",
        "\n",
        "# Define the BEIR tasks you want to evaluate on\n",
        "tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "# Evaluate the model on the benchmark\n",
        "evaluation = mteb.MTEB(tasks=tasks)\n",
        "results = evaluation.run(pca_model, output_folder=f\"results/{model_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + Unsupervised Matryoshka Adaptor"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
