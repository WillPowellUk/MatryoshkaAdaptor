{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdg4efAmPQ32"
      },
      "source": [
        "## Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aT1EJEfjPQ34"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNO5WkvKPQ35",
        "outputId": "decd2625-7e24-422e-e478-3f20bada4688"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt\n",
        "# !pip install sentence-transformers\n",
        "# !pip install mteb\n",
        "# !pip install beir\n",
        "# !pip install datasets\n",
        "# !pip install wandb\n",
        "# !pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwnKu7oPQ36"
      },
      "source": [
        "## Matryoshka-Adaptor Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuuAdgX7sD00"
      },
      "source": [
        "### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpi5vvq4sH80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define MatryoshkaAdaptor module - a simple MLP with skip connection\n",
        "class MatryoshkaAdaptor(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch neural network module that adapts the output of an embedding model\n",
        "    into a desired output dimension using two linear transformations with a ReLU activation in between.\n",
        "    Includes a skip connection from input to output.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_output_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initializes the MatryoshkaAdaptor module.\n",
        "        \n",
        "        Args:\n",
        "            input_output_dim: An integer representing the input and output dimension of the module which are equal.\n",
        "            hidden_dim: An integer representing the hidden dimension of the module.\n",
        "            \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        super(MatryoshkaAdaptor, self).__init__()\n",
        "        self.input_output_dim = input_output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # First linear layer to transform the input dimension to a hidden dimension\n",
        "        self.linear1 = torch.nn.Linear(input_output_dim, hidden_dim)\n",
        "        # Second linear layer to transform the hidden dimension to the output dimension which is same as input dimension\n",
        "        self.linear2 = torch.nn.Linear(hidden_dim, input_output_dim)\n",
        "        # Activation function to introduce non-linearity\n",
        "        self.activation = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, embedding):\n",
        "        \"\"\"\n",
        "        Forward pass of the MatryoshkaAdaptor module.\n",
        "\n",
        "        Args:\n",
        "            embedding: A torch.Tensor of shape (batch_size, input_output_dim) representing the input embeddings.\n",
        "\n",
        "        Returns:\n",
        "            output: A torch.Tensor of shape (batch_size, input_output_dim) representing the matryoshka embeddings.\n",
        "        \"\"\"\n",
        "        # Apply the first linear transformation followed by the activation function\n",
        "        hidden_embedding = self.activation(self.linear1(embedding))\n",
        "        \n",
        "        # Apply the second linear transformation to get the final adapted embedding\n",
        "        adapted_embedding = self.linear2(hidden_embedding)\n",
        "        \n",
        "        # Add the skip connection by adding the original embedding to the adapted embedding\n",
        "        mat_embedding = adapted_embedding + embedding\n",
        "\n",
        "        return mat_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i3hyw53tnoz"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_inssTutsBl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Equation 1 in paper\n",
        "def pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims):\n",
        "    \"\"\"\n",
        "    Computes the pairwise similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the mean pairwise similarity loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # Original embeddings only need to be normalized and computed once: \n",
        "\n",
        "    # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "    normalized_ori_corpus_embeddings = F.normalize(ori_corpus_embeddings, p=2, dim=1)\n",
        "    # Compute the cosine similarity matrices\n",
        "    original_similarity_matrix = torch.matmul(normalized_ori_corpus_embeddings, normalized_ori_corpus_embeddings.T)\n",
        "\n",
        "    # Get the indices of the upper triangle of the matrices, excluding the diagonal\n",
        "    batch_size = ori_corpus_embeddings.size(0)\n",
        "    i, j = torch.triu_indices(batch_size, batch_size, offset=1)\n",
        "\n",
        "    # Compute the pairwise cosine similarities\n",
        "    original_pairwise_similarities = original_similarity_matrix[i, j]\n",
        "\n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce the matryoshka embeddings to m dimensions\n",
        "        reduced_mat_corpus_embeddings = mat_corpus_embeddings[:, :m]\n",
        "\n",
        "        # Normalize the embeddings along the embedding dimension to get the cosine similarity\n",
        "        normalized_mat_corpus_embeddings = F.normalize(mat_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute the cosine similarity matrices\n",
        "        matryoshka_similarity_matrix = torch.matmul(normalized_mat_corpus_embeddings, normalized_mat_corpus_embeddings.T)\n",
        "        \n",
        "        # Compute the pairwise cosine similarities\n",
        "        matryoshka_pairwise_similarities = matryoshka_similarity_matrix[i, j]\n",
        "        \n",
        "        # Compute the absolute difference between corresponding pairwise similarities\n",
        "        similarity_differences = torch.abs(original_pairwise_similarities - matryoshka_pairwise_similarities)\n",
        "        \n",
        "        # Sum up all the absolute differences to produce the final loss\n",
        "        loss += torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Equation 2 in paper\n",
        "def topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k=10):\n",
        "    \"\"\"\n",
        "    Computes the top-k similarity loss between original embeddings and matryoshka embeddings.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similarities to consider (default is 10).\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the top-k similarity loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Normalize the original embeddings to get cosine similarity\n",
        "    normalized_ori_corpus_embeddings = F.normalize(ori_corpus_embeddings, p=2, dim=1)\n",
        "    \n",
        "    # Compute the original cosine similarity matrix\n",
        "    original_similarity_matrix = torch.matmul(normalized_ori_corpus_embeddings, normalized_ori_corpus_embeddings.T)\n",
        "    \n",
        "    # Exclude self-similarity by setting the diagonal to a very low value\n",
        "    original_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "    \n",
        "    # For each embedding, get the top-k similarities for the original embeddings\n",
        "    original_topk_values, _ = torch.topk(original_similarity_matrix, k, dim=1)\n",
        "    \n",
        "    loss = 0.0\n",
        "    for m in m_dims:\n",
        "        # Reduce the matryoshka embeddings to m dimensions\n",
        "        reduced_mat_corpus_embeddings = mat_corpus_embeddings[:, :m]\n",
        "\n",
        "        # Normalize the reduced matryoshka embeddings to get cosine similarity\n",
        "        normalized_mat_corpus_embeddings = F.normalize(reduced_mat_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute the cosine similarity matrix for the reduced embeddings\n",
        "        matryoshka_similarity_matrix = torch.matmul(normalized_mat_corpus_embeddings, normalized_mat_corpus_embeddings.T)\n",
        "        \n",
        "        # Exclude self-similarity by setting the diagonal to a very low value\n",
        "        matryoshka_similarity_matrix.fill_diagonal_(-float('inf'))\n",
        "        \n",
        "        # For each embedding, get the top-k similarities for the matryoshka embeddings\n",
        "        matryoshka_topk_values, _ = torch.topk(matryoshka_similarity_matrix, k, dim=1)\n",
        "        \n",
        "        # Compute the absolute difference between the top-k similarities\n",
        "        similarity_differences = torch.abs(original_topk_values - matryoshka_topk_values)\n",
        "        \n",
        "        # Sum up all the absolute differences to accumulate the final loss\n",
        "        loss += torch.sum(similarity_differences)\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Equation 3 in paper\n",
        "def reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Computes the reconstruction loss to ensure the matryoshka embeddings do not deviate\n",
        "    significantly from the original embeddings, and thus act as a regularizer.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        alpha: A reconstruction coefficient that controls the weight of the reconstruction term.\n",
        "        \n",
        "    Returns:\n",
        "        loss: A scalar tensor representing the reconstruction loss.\n",
        "    \"\"\"\n",
        "    # Compute the difference between original and matryoshka embeddings\n",
        "    diff = ori_corpus_embeddings - mat_corpus_embeddings\n",
        "    \n",
        "    # Compute the L2 norm of the difference\n",
        "    loss = torch.norm(diff, p=2, dim=1)\n",
        "    \n",
        "    # Return the mean loss over the batch, scaled by alpha\n",
        "    return alpha * loss.mean()\n",
        "\n",
        "\n",
        "# Equation 4 in paper\n",
        "def unsupervised_objective_fn_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims,\n",
        "                                   k=10, alpha=1.0, beta=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall unsupervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, and beta-scaled reconstruction loss.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka/adapted embeddings.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the reconstruction loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    topk_loss = topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k)\n",
        "    pairwise_loss = pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims)\n",
        "    reg_loss = reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, beta)\n",
        "    \n",
        "    # Combine the losses with the given scaling factors\n",
        "    total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "# Equation 5 in paper\n",
        "def matryoshka_ranking_loss(query_embeddings, corpus_embeddings, relevance_scores, m_dims, k=10):\n",
        "    \"\"\"\n",
        "    Computes the Matryoshka Ranking Loss using optimized matrix operations and normalization.\n",
        "    \n",
        "    Args:\n",
        "        query_embeddings (torch.Tensor): Query embeddings of shape (num_queries, embedding_dim).\n",
        "        corpus_embeddings (torch.Tensor): Corpus embeddings of shape (num_docs, embedding_dim).\n",
        "        relevance_scores (torch.Tensor): Relevance scores of shape (num_queries, num_docs).\n",
        "        m_dims (List[int]): List of reduced dimensionality values.\n",
        "        k (int): Number of top similar documents to consider for the loss.\n",
        "    \n",
        "    Returns:\n",
        "        torch.Tensor: The computed Matryoshka Ranking Loss.\n",
        "    \"\"\"\n",
        "    \n",
        "    total_loss = 0.0\n",
        "    num_queries = query_embeddings.size(0)\n",
        "    \n",
        "    for m in m_dims:\n",
        "        # Reduce embeddings to m dimensions\n",
        "        reduced_query_embeddings = query_embeddings[:, :m]\n",
        "        reduced_corpus_embeddings = corpus_embeddings[:, :m]\n",
        "        \n",
        "        # Normalize the embeddings to unit vectors\n",
        "        reduced_query_embeddings = F.normalize(reduced_query_embeddings, p=2, dim=1)\n",
        "        reduced_corpus_embeddings = F.normalize(reduced_corpus_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Compute cosine similarities\n",
        "        similarities = torch.matmul(reduced_query_embeddings, reduced_corpus_embeddings.T)\n",
        "        \n",
        "        # Get the top k most similar documents for each query\n",
        "        top_k_similarities, top_k_indices = torch.topk(similarities, k, dim=1, largest=True)\n",
        "        \n",
        "        # Gather the corresponding relevance scores\n",
        "        top_k_relevance = torch.gather(relevance_scores, 1, top_k_indices)\n",
        "        \n",
        "        # Calculate pairwise differences for the top-k relevance scores and similarities\n",
        "        relevance_diff = top_k_relevance.unsqueeze(2) - top_k_relevance.unsqueeze(1)\n",
        "        sim_diff = top_k_similarities.unsqueeze(2) - top_k_similarities.unsqueeze(1)\n",
        "        \n",
        "        # Only consider pairs where the relevance score difference is positive\n",
        "        positive_diff_mask = (relevance_diff > 0).float()\n",
        "        \n",
        "        # Compute the logistic loss with a numerically stable softplus\n",
        "        log_loss = F.softplus(sim_diff)\n",
        "        \n",
        "        # Weight the loss by the relevance difference and accumulate\n",
        "        weighted_loss = positive_diff_mask * relevance_diff * log_loss\n",
        "        total_loss += weighted_loss.sum() / num_queries  # Normalize by the number of queries\n",
        "    \n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "# Equation 6 in paper\n",
        "def supervised_objective_fn_loss(ori_query_embeddings, ori_corpus_embeddings, mat_query_embeddings, mat_corpus_embeddings, relevance_scores, m_dims,\n",
        "                                   k=5, alpha=1.0, beta=1.0, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Computes the overall supervised objective function loss as a combination of top-k similarity loss,\n",
        "    alpha-scaled pairwise similarity loss, beta-scaled reconstruction loss and gamma-scaled matryoshka ranking loss.\n",
        "    \n",
        "    Args:\n",
        "        ori_corpus_embeddings: A tensor of shape (batch_size, embedding_dim) representing the original embeddings.\n",
        "        mat_corpus_embeddings: A tensor of shape (batch_size, mat_embedding_dim) representing the matryoshka embeddings.\n",
        "        relevance_scores: A tensor of shape (batch_size, num_docs) representing the relevance scores.\n",
        "        m_dims: List of reduced matryoshka dimensionality values.\n",
        "        k: The number of top similar embeddings to consider for the top-k similarity loss.\n",
        "        alpha: A scaling factor for the pairwise similarity loss.\n",
        "        beta: A scaling factor for the reconstruction loss.\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: A scalar tensor representing the combined unsupervised objective function loss.\n",
        "    \"\"\"\n",
        "    # Compute the individual loss components\n",
        "    #topk_loss = topk_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims, k)\n",
        "    #pairwise_loss = pairwise_similarity_loss(ori_corpus_embeddings, mat_corpus_embeddings, m_dims)\n",
        "    #reg_loss = reconstruction_loss(ori_corpus_embeddings, mat_corpus_embeddings, beta)\n",
        "    ranking_loss = matryoshka_ranking_loss(ori_corpus_embeddings, mat_corpus_embeddings, relevance_scores, m_dims)\n",
        "\n",
        "    # Combine the losses with the given scaling factors\n",
        "    #total_loss = topk_loss + alpha * pairwise_loss + beta * reg_loss + gamma * ranking_loss\n",
        "    total_loss = ranking_loss\n",
        "    \n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train Function\n",
        "\n",
        "This train function is designed for both unsupervised and supervised methods to train the Matryoshka Adaptor (not the embedding model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "import wandb\n",
        "import tqdm\n",
        "\n",
        "def train(model, mat_adaptor, train_loader, loss_fn, config, run_name):\n",
        "    \"\"\"\n",
        "    Trains the MatryoshkaAdaptor module using the provided training data.\n",
        "\n",
        "    Args:\n",
        "        model: A SentenceTransformer model to generate embeddings.\n",
        "        mat_adaptor: A MatryoshkaAdaptor module to adapt the embeddings.\n",
        "        train_loader: A DataLoader object for the training dataset.\n",
        "        loss_fn: A loss function to compute the loss between original and matryoshka embeddings.\n",
        "        config: A dictionary containing hyperparameters for training.\n",
        "        run_name: A string representing the name of the save path run.\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Detect if CUDA is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Move the model and mat_adaptor to the device\n",
        "    model.to(device)\n",
        "    mat_adaptor.to(device)\n",
        "\n",
        "    # Unpack the hyperparameters\n",
        "    epochs = config.get('epochs', 5)\n",
        "    lr = config.get('lr', 1e-3)\n",
        "    k = config.get('k', 10)  # Top-k similarity loss\n",
        "    m_dims = config.get('m_dims', [64, 128, 256])  # Matryoshka embedding dimensions\n",
        "    alpha = config.get('alpha', 1.0)  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    beta = config.get('beta', 1.0)  # Reconstruction loss scaling factor (beta in paper)\n",
        "    gamma = config.get('gamma', 1.0)  # Ranking loss scaling factor (gamma in paper)\n",
        "\n",
        "    # Initialize Weights & Biases\n",
        "    if config.get('wandb', False):\n",
        "        wandb.init(project=\"matryoshka-training\", config=config)\n",
        "        config = wandb.config\n",
        "\n",
        "    # Define an optimizer for the MatryoshkaAdaptor parameters\n",
        "    optimizer = Adam(mat_adaptor.parameters(), lr=lr)\n",
        "\n",
        "    # Set embedding model to eval mode (so that gradients only apply to the MatryoshkaAdaptor)\n",
        "    model.eval()\n",
        "    \n",
        "    # Set MatryoshkaAdaptor to training mode\n",
        "    mat_adaptor.train()\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Loop over batches in the current epoch\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False):\n",
        "            if isinstance(batch, list):\n",
        "                ori_embeddings = model.encode(batch, convert_to_tensor=True).to(device)  # model batched embeddings\n",
        "                mat_embeddings = mat_adaptor(ori_embeddings)\n",
        "                loss = loss_fn(ori_embeddings, mat_embeddings, m_dims, k=k, alpha=alpha, beta=beta)\n",
        "\n",
        "            elif isinstance(batch, dict):\n",
        "                queries, corpus, relevance_scores = batch['query'], batch['corpus'], batch['relevance'].to(device)\n",
        "                ori_query_embeddings = model.encode([queries, corpus], convert_to_tensor=True).to(device)\n",
        "                ori_corpus_embeddings = model.encode(corpus, convert_to_tensor=True).to(device)\n",
        "                mat_query_embeddings = mat_adaptor(ori_query_embeddings).to(device)\n",
        "                mat_corpus_embeddings = mat_adaptor(ori_corpus_embeddings).to(device)\n",
        "                loss = loss_fn(ori_query_embeddings, ori_corpus_embeddings, mat_query_embeddings, mat_corpus_embeddings, relevance_scores, m_dims, k=k, alpha=alpha, beta=beta, gamma=gamma)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid batch format. Please provide a list or dictionary.\")\n",
        "\n",
        "            optimizer.zero_grad()  # Clear previous gradients\n",
        "            loss.backward()        # Compute gradients\n",
        "            optimizer.step()       # Update weights\n",
        "\n",
        "            print(F\"Loss: {loss.item():.4f}\")\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        \n",
        "        # Log the average loss to W&B\n",
        "        if config.get('wandb', False):\n",
        "            wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
        "        \n",
        "        # Print average loss for the epoch\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Save model checkpoint every 5 epochs and on the final epoch\n",
        "        if (epoch + 1) % 5 == 0 or (epoch + 1) == epochs:\n",
        "            checkpoint_path = f\"{run_name}_epoch_{epoch+1}.pt\"\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': mat_adaptor.state_dict(),\n",
        "                'input_output_dim': mat_adaptor.input_output_dim,\n",
        "                'hidden_dim': mat_adaptor.hidden_dim,\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path}\")\n",
        "\n",
        "    # Final save (this is optional if the final epoch is a multiple of 5)\n",
        "    final_checkpoint_path = f\"{run_name}_epoch_{epochs}_final.pt\"\n",
        "    torch.save({\n",
        "        'epoch': epochs,\n",
        "        'model_state_dict': mat_adaptor.state_dict(),\n",
        "        'input_output_dim': mat_adaptor.input_output_dim,\n",
        "        'hidden_dim': mat_adaptor.hidden_dim,\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_loss,\n",
        "    }, final_checkpoint_path)\n",
        "    print(f\"Final checkpoint saved at {final_checkpoint_path}\")\n",
        "    \n",
        "\n",
        "    # Finish the W&B run\n",
        "    if config.get('wandb', False):\n",
        "        wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q9GK1c80AGX"
      },
      "source": [
        "## Training of Adaptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsb8bFydrejN"
      },
      "source": [
        "### Unsupervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wfOS9tZEgZ0"
      },
      "source": [
        "#### Prepare Datasets \n",
        "\n",
        "We will use BEIR's NFCorpus, and train on the corpus only in an unsupervised manner, as detailed in the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tf7kTmMfsrUM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "# ds = load_dataset(\"BeIR/nfcorpus\", \"corpus\")\n",
        "ds = load_dataset(\"BeIR/webis-touche2020\", \"corpus\")\n",
        "\n",
        "# Access the 'corpus' dataset\n",
        "dataset = ds['corpus']['text']\n",
        "\n",
        "# Define the split sizes\n",
        "train_size = int(0.7 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ccU_oZwyPQ37",
        "outputId": "8da10a58-7ccc-49bb-ef60-c1ff58a6a939"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 1e-3,\n",
        "    'batch_size': 128,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm_dims': [64, 128, 256],  # Matryoshka embedding dimensions\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0,  # reconstruction loss scaling factor (beta in paper)\n",
        "    'wandb': False\n",
        "}\n",
        "\n",
        "# Create DataLoader for train and test datasets\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
        "\n",
        "run_name = \"ckpts/unsupervised_ma\"\n",
        "train(model, mat_adaptor, train_dataloader, unsupervised_objective_fn_loss, hyperparams, run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Datasets\n",
        "\n",
        "As before, we will use BEIR's NFCorpus, and train on the corpus-query pairs in a supervised manner, as detailed in the paper.\n",
        "We need to manually download the BEIR dataset since the qrels cannot be accessed on the huggingface dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from beir import util, LoggingHandler\n",
        "from beir.datasets.data_loader import GenericDataLoader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s', level=logging.INFO, handlers=[LoggingHandler()])\n",
        "\n",
        "# Define the dataset name and the path to store it\n",
        "dataset = \"nfcorpus\"\n",
        "url = f\"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{dataset}.zip\"\n",
        "data_path = util.download_and_unzip(url, \"datasets\")\n",
        "\n",
        "# Load train and test data\n",
        "train_corpus, train_queries, train_qrels = GenericDataLoader(data_path).load(split=\"train\")\n",
        "dev_corpus, dev_queries, dev_qrels = GenericDataLoader(data_path).load(split=\"dev\")\n",
        "test_corpus, test_queries, test_qrels = GenericDataLoader(data_path).load(split=\"test\")\n",
        "\n",
        "class BEIRDataset(Dataset):\n",
        "    def __init__(self, query, corpus, qrels):\n",
        "        self.query = query\n",
        "        self.corpus = corpus\n",
        "        self.qrels = qrels\n",
        "        self.query_ids = list(query.keys())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.query_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        query_id = self.query_ids[idx]\n",
        "        query = self.query[query_id]\n",
        "        \n",
        "        relevant_docs = self.qrels.get(query_id, {})\n",
        "        \n",
        "        # Get all document ids and relevance for this query\n",
        "        corpus_ids = list(relevant_docs.keys())\n",
        "        relevance = [relevant_docs[doc_id] for doc_id in corpus_ids]\n",
        "        \n",
        "        # Get document texts\n",
        "        corpus = [self.corpus[doc_id] for doc_id in corpus_ids]\n",
        "        \n",
        "        return {\n",
        "            'query_id': query_id,\n",
        "            'query': query,\n",
        "            'corpus_ids': corpus_ids,\n",
        "            'corpus': corpus,\n",
        "            'relevance': relevance\n",
        "        }\n",
        "    \n",
        "\n",
        "def collate_fn(batch):\n",
        "    query_ids = [item['query_id'] for item in batch]\n",
        "    query = [item['query'] for item in batch]\n",
        "    corpus_ids = [item['corpus_ids'] for item in batch]\n",
        "    corpus = [item['corpus'] for item in batch]\n",
        "    relevance = [item['relevance'] for item in batch]\n",
        "    \n",
        "    # Pad sequences if necessary\n",
        "    max_docs = max(len(docs) for docs in corpus_ids)\n",
        "    \n",
        "    padded_corpus_ids = [docs + [''] * (max_docs - len(docs)) for docs in corpus_ids]\n",
        "    padded_corpus = [texts + [''] * (max_docs - len(texts)) for texts in corpus]\n",
        "    padded_relevance = [s + [0] * (max_docs - len(s)) for s in relevance]\n",
        "    \n",
        "    return {\n",
        "        'query_ids': query_ids,\n",
        "        'query': query,\n",
        "        'corpus_ids': padded_corpus_ids,  # Keep as a list of lists\n",
        "        'corpus': padded_corpus,  # Keep as a list of lists\n",
        "        'relevance': torch.tensor(padded_relevance)  # Convert only relevance to tensor\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare Embedding Model and Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Embedding Model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Matryoshka-Adaptor\n",
        "input_output_dim = model.get_sentence_embedding_dimension() # Embedding dimension for model (d in paper)\n",
        "hidden_dim = input_output_dim # Let hidden layer dimension equal the embedding model dimension\n",
        "mat_adaptor = MatryoshkaAdaptor(input_output_dim, hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train Matryoshka-Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperparams = {\n",
        "    'epochs': 5,\n",
        "    'lr': 5e-4,\n",
        "    'batch_size': 128,\n",
        "    'k': 5,  # Top-k similarity loss\n",
        "    'm_dims': [64, 128, 256],  # Matryoshka embedding dimension\n",
        "    'alpha': 1.0,  # Pairwise similarity loss scaling factor (alpha in paper)\n",
        "    'beta': 1.0,  # reconstruction loss scaling factor (beta in paper)\n",
        "    'gamma': 1.0,  # Ranking loss scaling factor (gamma in paper)\n",
        "    'wandb': False,\n",
        "}\n",
        "\n",
        "train_dataset = BEIRDataset(train_queries, train_corpus, train_qrels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=hyperparams['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "# dev_dataset = BEIRDataset(dev_queries, dev_corpus, dev_qrels)\n",
        "# dev_dataloader = DataLoader(dev_dataset, batch_size=hyperparams['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# test_dataset = BEIRDataset(test_queries, test_corpus, test_qrels)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=hyperparams['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "run_name = \"ckpts/supervised_ma\"\n",
        "train(model, mat_adaptor, train_dataloader, supervised_objective_fn_loss, hyperparams, run_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhFgceqlt8xx"
      },
      "source": [
        "## BEIR Evaluation using NFCorpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84bFC2eMSAQW"
      },
      "source": [
        "### Unmodified Model Performance for all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Rudimentary SentenceTransformer subclass to crop embeddings\n",
        "class CropSentenceTransformer(SentenceTransformer):\n",
        "    def __init__(self, model_name_or_path, m):\n",
        "        super(CropSentenceTransformer, self).__init__(model_name_or_path)\n",
        "        self.model = SentenceTransformer(model_name_or_path)\n",
        "        self.m = m\n",
        "\n",
        "    def encode(self, sentences, **kwargs):\n",
        "        # Get the embeddings from the embedding model\n",
        "        embeddings = self.model.encode(sentences, **kwargs)\n",
        "\n",
        "        # Crop the embeddings to the desired dimension m\n",
        "        reduced_embeddings = embeddings[:, :self.m]\n",
        "        \n",
        "        return reduced_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "LlrIjQBAs7b2",
        "outputId": "d7589094-fb96-42d5-b90a-59ca05231210"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #262626; text-decoration-color: #262626\">───────────────────────────────────────────────── </span><span style=\"font-weight: bold\">Selected tasks </span><span style=\"color: #262626; text-decoration-color: #262626\"> ─────────────────────────────────────────────────</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[38;5;235m───────────────────────────────────────────────── \u001b[0m\u001b[1mSelected tasks \u001b[0m\u001b[38;5;235m ─────────────────────────────────────────────────\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Retrieval</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mRetrieval\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">    - Touche2020, <span style=\"color: #626262; text-decoration-color: #626262; font-style: italic\">s2p</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "    - Touche2020, \u001b[3;38;5;241ms2p\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n",
            "Batches:   4%|▎         | 14/391 [02:27<1:06:23, 10.57s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the benchmark\u001b[39;00m\n\u001b[1;32m     11\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m mteb\u001b[38;5;241m.\u001b[39mMTEB(tasks\u001b[38;5;241m=\u001b[39mtasks)\n\u001b[0;32m---> 12\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/Touche2020/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mm\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_splits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/evaluation/MTEB.py:383\u001b[0m, in \u001b[0;36mMTEB.run\u001b[0;34m(self, model, verbosity, output_folder, eval_splits, overwrite_results, raise_error, co2_tracker, encode_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m     kg_co2_emissions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    380\u001b[0m         tracker\u001b[38;5;241m.\u001b[39mfinal_emissions\n\u001b[1;32m    381\u001b[0m     )  \u001b[38;5;66;03m# expressed as kilograms of CO₂-equivalents\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     results, tick, tock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;241m.\u001b[39mmetadata_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtock\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mtick\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m )\n\u001b[1;32m    395\u001b[0m evaluation_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tock \u001b[38;5;241m-\u001b[39m tick\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/evaluation/MTEB.py:260\u001b[0m, in \u001b[0;36mMTEB._run_eval\u001b[0;34m(task, model, split, output_folder, encode_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_eval\u001b[39m(\n\u001b[1;32m    251\u001b[0m     task: AbsTask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    258\u001b[0m ):\n\u001b[1;32m    259\u001b[0m     tick \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 260\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     tock \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results, tick, tock\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/abstasks/AbsTaskRetrieval.py:299\u001b[0m, in \u001b[0;36mAbsTaskRetrieval.evaluate\u001b[0;34m(self, model, split, encode_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         corpus, queries, relevant_docs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus[hf_subset][split],\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueries[hf_subset][split],\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevant_docs[hf_subset][split],\n\u001b[1;32m    298\u001b[0m         )\n\u001b[0;32m--> 299\u001b[0m     scores[hf_subset] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_subset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevant_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/abstasks/AbsTaskRetrieval.py:308\u001b[0m, in \u001b[0;36mAbsTaskRetrieval._evaluate_subset\u001b[0;34m(self, retriever, corpus, queries, relevant_docs, hf_subset, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_subset\u001b[39m(\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m, retriever, corpus, queries, relevant_docs, hf_subset: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ScoresDict:\n\u001b[1;32m    307\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 308\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    310\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken to retrieve: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/evaluation/evaluators/RetrievalEvaluator.py:488\u001b[0m, in \u001b[0;36mRetrievalEvaluator.__call__\u001b[0;34m(self, corpus, queries)\u001b[0m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m    481\u001b[0m         corpus,\n\u001b[1;32m    482\u001b[0m         queries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         prompt_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/evaluation/evaluators/RetrievalEvaluator.py:153\u001b[0m, in \u001b[0;36mDenseRetrievalExactSearch.search\u001b[0;34m(self, corpus, queries, top_k, score_function, prompt_name, instructions, request_qid, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     sub_corpus_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings[request_qid][batch_num]\n\u001b[1;32m    150\u001b[0m     )\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Encode chunk of corpus\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     sub_corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_corpus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorpus_start_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mcorpus_end_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_qid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_corpus_embeddings \u001b[38;5;129;01mand\u001b[39;00m request_qid:\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings[request_qid]\u001b[38;5;241m.\u001b[39mappend(sub_corpus_embeddings)\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/evaluation/evaluators/RetrievalEvaluator.py:395\u001b[0m, in \u001b[0;36mDRESModel.encode_corpus\u001b[0;34m(self, corpus, prompt_name, batch_size, request_qid, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    389\u001b[0m         (doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m doc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus\n\u001b[1;32m    393\u001b[0m     ]\n\u001b[0;32m--> 395\u001b[0m corpus_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_corpus_embeddings \u001b[38;5;129;01mand\u001b[39;00m request_qid:\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_embeddings[request_qid] \u001b[38;5;241m=\u001b[39m corpus_embeddings\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/mteb/evaluation/evaluators/model_encode.py:40\u001b[0m, in \u001b[0;36mmodel_encode\u001b[0;34m(sentences, model, prompt_name, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     42\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n",
            "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mCropSentenceTransformer.encode\u001b[0;34m(self, sentences, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Get the embeddings from the embedding model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Crop the embeddings to the desired dimension m\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     reduced_embeddings \u001b[38;5;241m=\u001b[39m embeddings[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm]\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:517\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    514\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 517\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    519\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:118\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    116\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    121\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    981\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    982\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    983\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    987\u001b[0m )\n\u001b[0;32m--> 988\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    572\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    573\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         output_attentions,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:411\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    402\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    403\u001b[0m         hidden_states,\n\u001b[1;32m    404\u001b[0m         attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m         output_attentions,\n\u001b[1;32m    410\u001b[0m     )\n\u001b[0;32m--> 411\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:361\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 361\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    363\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/ML_Dev/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import mteb\n",
        "m_dims = [64, 128, 256, 384]\n",
        "for m in m_dims:\n",
        "    model = CropSentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', m)\n",
        "    model_name = 'all-MiniLM-L6-v2_BASE'\n",
        "\n",
        "    # Define the BEIR tasks you want to evaluate on\n",
        "    tasks = mteb.get_tasks(tasks=[\"Touche2020\"])\n",
        "\n",
        "    # Evaluate the model on the benchmark\n",
        "    evaluation = mteb.MTEB(tasks=tasks)\n",
        "    results = evaluation.run(model, output_folder=f\"results/Touche2020/{model_name}/{m}\", eval_splits=[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "class PCASentenceTransformer(SentenceTransformer):\n",
        "    \"\"\"\n",
        "    A SentenceTransformer model that applies PCA to reduce the dimensionality of the embeddings. \n",
        "    It serves as a wrapper to the inputted SentenceTransformer. \n",
        "    \"\"\"\n",
        "    def __init__(self, model_name_or_path, pca_components=128):\n",
        "        super().__init__(model_name_or_path)\n",
        "        self.pca_components = pca_components\n",
        "        self.pca = None\n",
        "\n",
        "    def fit_pca(self, embeddings):\n",
        "        \"\"\"Fits PCA on the provided embeddings.\"\"\"\n",
        "        self.pca = PCA(n_components=self.pca_components)\n",
        "        self.pca.fit(embeddings)\n",
        "\n",
        "    def encode(self, sentences, **kwargs):\n",
        "        \"\"\"Encodes the sentences and applies PCA to reduce dimensions.\"\"\"\n",
        "        # First, get the embeddings from the parent class method\n",
        "        embeddings = super().encode(sentences, **kwargs)\n",
        "\n",
        "        # If PCA is not fitted, fit it using the embeddings\n",
        "        if self.pca is None:\n",
        "            self.fit_pca(embeddings)\n",
        "\n",
        "        # Transform the embeddings using the fitted PCA model\n",
        "        reduced_embeddings = self.pca.transform(embeddings)\n",
        "        return reduced_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mteb\n",
        "\n",
        "m_dims = [64, 128, 256, 384]\n",
        "for m in m_dims:\n",
        "    # Create PCA model\n",
        "    pca_model = PCASentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', pca_components=m)\n",
        "    model_name = f'all-MiniLM-L6-v2_PCA_{m}'\n",
        "\n",
        "    # Ensure the model is on the right device\n",
        "    pca_model.to('cpu') # sklearn PCA does not support GPU\n",
        "\n",
        "    # Define the BEIR tasks you want to evaluate on\n",
        "    tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "    # Evaluate the model on the benchmark\n",
        "    evaluation = mteb.MTEB(tasks=tasks)\n",
        "    \n",
        "    # Ensure the model's output is on the CPU before passing to numpy\n",
        "    results = evaluation.run(pca_model, output_folder=f\"/content/drive/MyDrive/results/{model_name}/{m}\", eval_splits=[\"test\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### all-MiniLM-L6-v2 + Unsupervised Matryoshka Adaptor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "class MASentenceTransformer(SentenceTransformer):\n",
        "    def __init__(self, model_name_or_path, ma_ckpt, m):\n",
        "        super(MASentenceTransformer, self).__init__(model_name_or_path)\n",
        "        self.model = SentenceTransformer(model_name_or_path)\n",
        "        self.m = m\n",
        "\n",
        "        # Matryoshka-Adaptor\n",
        "        self.mat_adaptor = MatryoshkaAdaptor(self.model.get_sentence_embedding_dimension(), self.model.get_sentence_embedding_dimension())\n",
        "\n",
        "        # Load the state dictionary from the checkpoint\n",
        "        self.mat_adaptor.load_state_dict(torch.load(ma_ckpt)['model_state_dict'])\n",
        "\n",
        "    def encode(self, sentences, **kwargs):\n",
        "        # Get the embeddings from the embedding model\n",
        "        embeddings = self.model.encode(sentences, **kwargs)\n",
        "\n",
        "        # Transform the embeddings using the Matryoshka Adaptor\n",
        "        reduced_embeddings = self.mat_adaptor(embeddings)\n",
        "\n",
        "        # Crop the embeddings to the desired dimension m\n",
        "        reduced_embeddings = reduced_embeddings[:, :self.m]\n",
        "        \n",
        "        return reduced_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mteb\n",
        "\n",
        "ma_ckpt = \"ckpts/unsupervised_ma_epoch_8_final.pt\"\n",
        "# m_dims = [64, 128, 256, 384]\n",
        "m_dims = [384]\n",
        "for m in m_dims:\n",
        "    ma_model = MASentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', ma_ckpt=ma_ckpt, m=m)\n",
        "    model_name = f'all-MiniLM-L6-v2_MAT_ADAPTOR_UNSUPERVISED'\n",
        "\n",
        "    # Define the BEIR tasks you want to evaluate on\n",
        "    tasks = mteb.get_tasks(tasks=[\"NFCorpus\"])\n",
        "\n",
        "    # Evaluate the model on the benchmark\n",
        "    evaluation = mteb.MTEB(tasks=tasks)\n",
        "    results = evaluation.run(ma_model.to('cpu'), output_folder=f\"/content/drive/MyDrive/results/{model_name}/{m}\", eval_splits=[\"test\"])\n",
        "    print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# BASE results\n",
        "base_result_paths = {\n",
        "    \"64\": \"results/all-MiniLM-L6-v2_BASE/64/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"128\": \"results/all-MiniLM-L6-v2_BASE/128/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"256\": \"results/all-MiniLM-L6-v2_BASE/256/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"384\": \"results/all-MiniLM-L6-v2_BASE/384/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\"\n",
        "}\n",
        "\n",
        "# PCA results\n",
        "pca_result_paths = {\n",
        "    \"64\": \"results/all-MiniLM-L6-v2_PCA/64/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"128\": \"results/all-MiniLM-L6-v2_PCA/128/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"256\": \"results/all-MiniLM-L6-v2_PCA/256/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"384\": \"results/all-MiniLM-L6-v2_BASE/384/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "}\n",
        "\n",
        "# Unsupervised Matryoshka-Adaptor results\n",
        "unsup_mat_adaptor_result_paths = {\n",
        "    \"64\": \"results/all-MiniLM-L6-v2_MAT_ADAPTOR_UNSUPERVISED/64/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"128\": \"results/all-MiniLM-L6-v2_MAT_ADAPTOR_UNSUPERVISED/128/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    \"256\": \"results/all-MiniLM-L6-v2_MAT_ADAPTOR_UNSUPERVISED/256/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\",\n",
        "    # \"384\": \"results/all-MiniLM-L6-v2_MAT_ADAPTOR_UNSUPERVISED/384/sentence-transformers__all-MiniLM-L6-v2/8b3219a92973c328a8e22fadcfa821b5dc75636a/NFCorpus.json\"\n",
        "}\n",
        "\n",
        "def load_results(result_paths):\n",
        "    dimensions = []\n",
        "    ndcg_at_10_values = []\n",
        "    \n",
        "    for dim, path in result_paths.items():\n",
        "        with open(path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            # Extract the NDCG@10 score\n",
        "            ndcg_at_10 = data['scores']['test'][0]['ndcg_at_10']\n",
        "            dimensions.append(int(dim))\n",
        "            ndcg_at_10_values.append(ndcg_at_10)\n",
        "    \n",
        "    return dimensions, ndcg_at_10_values\n",
        "\n",
        "# Load the results for each method\n",
        "base_dimensions, base_ndcg_at_10_values = load_results(base_result_paths)\n",
        "pca_dimensions, pca_ndcg_at_10_values = load_results(pca_result_paths)\n",
        "unsup_mat_adaptor_dimensions, unsup_mat_adaptor_ndcg_at_10_values = load_results(unsup_mat_adaptor_result_paths)\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot BASE\n",
        "plt.plot(base_dimensions, base_ndcg_at_10_values, marker='o', label='BASE')\n",
        "\n",
        "# Plot PCA\n",
        "plt.plot(pca_dimensions, pca_ndcg_at_10_values, marker='o', label='PCA')\n",
        "\n",
        "# Plot Unsupervised Matryoshka-Adaptor\n",
        "plt.plot(unsup_mat_adaptor_dimensions, unsup_mat_adaptor_ndcg_at_10_values, marker='o', label='MAT Adaptor (Unsupervised)')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xticks(range(0, max(base_dimensions) + 64, 64))\n",
        "plt.xlim(min(base_dimensions)-10, max(base_dimensions) + 10)\n",
        "plt.title('NDCG@10 vs Embedding Dimension For all-MiniLM-L6-v2')\n",
        "plt.xlabel('Embedding Dimension')\n",
        "plt.ylabel('NDCG@10')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
